{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [Spring 2022] CS 4361 / 5361 - Lab 3 - Embeddings for Text",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Before you start**\n",
        "\n",
        "Make a copy of this Colab by clicking on File > Save a Copy in Drive\n",
        "\n",
        "After making a copy, add your student id, last name, and first name to the title."
      ],
      "metadata": {
        "id": "i_i_Aum_QQBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_name = \"Salvador Robles Herrera\"\n",
        "student_id = \"80683116\""
      ],
      "metadata": {
        "id": "FH5xoZVmQR7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKsRDH5ZUdfasdv"
      },
      "source": [
        "# Embeddings for Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43534tdfgs-v"
      },
      "source": [
        "In this lab, we'll train models for sentiment classification and experiment with learned embeddings for text features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY7emKinFRCr"
      },
      "source": [
        "### Objectives\n",
        "* Understand the basics of text processing using variable-length inputs.\n",
        "* Understand *sparse* vs. *one-hot* representations.\n",
        "* Compare models that use one-hot representations with models that use learned embeddings.\n",
        "* Build intuition for what is learned in the embedding parameters.\n",
        "* Practice setting up models in Keras using layers for concatenation, averaging, embeddings, and fully-connected feed-forward connections."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gKL1mrkyLux"
      },
      "source": [
        "### Grading\n",
        "\n",
        "This assignment (exercises 1-5) are worth a total of 100 points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style=\"darkgrid\")  # default style\n",
        "import plotly.graph_objs as plotly  # for interactive plots\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import imdb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqppUDpmdptk"
      },
      "source": [
        "## Data for Sentiment Classification\n",
        "\n",
        "In this lab, we'll train a *sentiment* classifier for movie reviews. That is, the input is the text of a movie review and the output is the probability the input was a positive review. The target labels are binary, 0 for negative and 1 for positive.\n",
        "\n",
        "Our data includes 50,000 movie reviews on IMDB. The data comes pre-segmented into train and test splits. The [data loading function](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data) below also splits each input text into tokens (words) and maps the words to integer values. Each input is a sequence of integers corresponding to the words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6M-asvhQWV_"
      },
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(path=\"imdb.npz\",\n",
        "                                                      num_words=None,\n",
        "                                                      skip_top=0,\n",
        "                                                      maxlen=None,\n",
        "                                                      seed=113,\n",
        "                                                      start_char=1,\n",
        "                                                      oov_char=2,\n",
        "                                                      index_from=3)\n",
        "\n",
        "print(\"X_train.shape:\", X_train.shape)\n",
        "print(\"Y_train.shape:\", Y_train.shape)\n",
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"Y_test.shape:\", Y_test.shape)\n",
        "\n",
        "print('First training example data:', X_train[0])\n",
        "print('First training example label:', Y_train[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Added to see shapes and get more info\n",
        "print(\"Number of words in example 0:\")\n",
        "print(len(X_train[0]))\n",
        "\n",
        "print(\"Number of words in example 1:\")\n",
        "print(len(X_train[1]))\n",
        "\n",
        "print(\"Number of words in example 2:\")\n",
        "print(len(X_train[2]))"
      ],
      "metadata": {
        "id": "_nI3DVBqq-TU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyIWiy-4gQK-"
      },
      "source": [
        "So our first training example is a positive review. But that sequence of integer IDs is hard to read. The data loader provides a dictionary mapping words to IDs. Let's create a reverse index.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQ-qATkhUj7c"
      },
      "source": [
        "# The imdb dataset comes with an index mapping words to integers.\n",
        "# In the index the words are ordered by frequency they occur.\n",
        "index = imdb.get_word_index()\n",
        "\n",
        "# Because we used index_from=3 (above), setting aside ids below 3 for special\n",
        "# symbols, we need to add 3 to the index values.\n",
        "index = dict([(key, value+3) for (key, value) in index.items()])\n",
        "\n",
        "# Create a reverse index so we can lookup tokens assigned to each id.\n",
        "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
        "reverse_index[1] = '<START>'  # start of input\n",
        "reverse_index[2] = '#'        # out-of-vocabulary (OOV)\n",
        "reverse_index[3] = '<UNUSED>'\n",
        "\n",
        "max_id = max(reverse_index.keys())\n",
        "print('Largest ID:', max_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h76-b07ehWNQ"
      },
      "source": [
        "Note that our index (and reverse index) have over 88,000 tokens. That's quite a large vocabulary! Let's also write a decoding function for our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjobmouHS5Dm"
      },
      "source": [
        "def decode(token_ids):\n",
        "  \"\"\"Return a string with the decoded text given a list of token ids.\"\"\"\n",
        "  # Try looking up each id in the index, but return '#' (for OOV) if not found.\n",
        "  tokens = [reverse_index.get(i, \"#\") for i in token_ids]\n",
        "\n",
        "  # Connect the string tokens with a space.\n",
        "  return ' '.join(tokens)\n",
        "\n",
        "# Show the ids corresponding tokens in the first example.\n",
        "print(X_train[0])\n",
        "print(decode(X_train[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g47w5CackGBA"
      },
      "source": [
        "### Text Lengths\n",
        "As usual, let's start with some data analysis. How long are the reviews? Is there a difference in length between positive and negative reviews? A histogram will help answer these questions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEOgzo8Gk3r7"
      },
      "source": [
        "# Create a list of lengths for training examples with a positive label.\n",
        "text_lengths_pos = [len(x) for (i, x) in enumerate(X_train) if Y_train[i]]\n",
        "\n",
        "# And a list of lengths for training examples with a negative label.\n",
        "text_lengths_neg = [len(x) for (i, x) in enumerate(X_train) if not Y_train[i]]\n",
        "\n",
        "# The histogram function can take a list of inputs and corresponding labels.\n",
        "plt.hist([text_lengths_pos, text_lengths_neg], bins=20, range=(0, 1000),\n",
        "         label=['positive', 'negative'])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Also check the longest reviews.\n",
        "print('Longest positive review:', max(text_lengths_pos))\n",
        "print('Longest negative review:', max(text_lengths_neg))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3ZE9gpkml3a"
      },
      "source": [
        "### Exercise 1: Token Counts (20 points)\n",
        "For each of the given tokens, construct a table with the number of positive training examples that include that token and the number of negative training examples that include that token. For reference, here are the counts for the first two tokens:\n",
        "\n",
        "|Token|Pos Count|Neg Count|\n",
        "|-|-|-|\n",
        "|good|4767|4849|\n",
        "|bad|1491|4396|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D6rGR1c1XQH"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YOYo6d01aWI"
      },
      "source": [
        "tokens = ['good', 'bad', 'amazing', 'boring', 'laugh', 'cry']\n",
        "# YOUR CODE HERE\n",
        "\n",
        "table_tokens = np.zeros((6,2))\n",
        "\n",
        "print(table_tokens)\n",
        "\n",
        "for rev in range(len(X_train)):\n",
        "  for tok in range(len(tokens)):\n",
        "    if index[tokens[tok]] in X_train[rev]:\n",
        "      if Y_train[rev] == 1:\n",
        "        table_tokens[tok][0] += 1\n",
        "      else:\n",
        "        table_tokens[tok][1] += 1\n",
        "\n",
        "print('Printed Number of positive and negative tokens in order from word good to cry')\n",
        "print(table_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhzt-LnQ1m8w"
      },
      "source": [
        "## Feature Representation\n",
        "Consider the difference between the pixel features we used for image classification and the text features we are now dealing with.\n",
        "\n",
        "An image had 784 pixel positions. At each position, there is a single value in [0,1] (after normalization).\n",
        "\n",
        "In contrast, a review has a variable number of ordered tokens (up to 2494 in the training examples). Each token occurs in a particular position. We can think of the token positions much like the 784 pixel positions, except that some of the trailing positions are empty, since review lengths vary.  At each token position, there is a single token, one of the 88587 entries in the vocabulary. So we can think of a review as a (2500, 90000) matrix: At each of ~2500 token positions, we have 1 of ~90000 token ids.\n",
        "\n",
        "This representation would have 2500 * 90000 = 225 million features -- quite a lot more complexity than the images, though as you'll see below, we will make some simplifying assumptions, reducing both the number of token positions and the number of vocabulary items."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm_F5JmWyfko"
      },
      "source": [
        "### Padding and Reduced Length\n",
        "As is clear from the length histogram, the current representation of the review text is a variable-length array. Since fixed-length arrays are easier to work with in Tensorflow, let's add special padding tokens at the end of each review until they are all the same length.\n",
        "\n",
        "We'll also use this operation to limit the number of token positions by truncating all reviews to a specified length. In the code below, as an example, we pad all training inputs to length 300."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4ou8bSUCWOx"
      },
      "source": [
        "def pad_data(sequences, max_length):\n",
        "  # Keras has a convenient utility for padding a sequence.\n",
        "  # Also make sure we get a numpy array rather than an array of lists.\n",
        "  return np.array(list(\n",
        "      tf.keras.preprocessing.sequence.pad_sequences(\n",
        "          sequences, maxlen=max_length, padding='post', value=0)))\n",
        "\n",
        "# Pad and truncate to 300 tokens.\n",
        "X_train_padded = pad_data(X_train, max_length=300)\n",
        "\n",
        "# Check the padded output.\n",
        "print('Length of X_train[0]:', len(X_train[0]))\n",
        "print('Length of X_train_padded[0]:', len(X_train_padded[0]))\n",
        "print(X_train_padded[0])\n",
        "\n",
        "print(X_train_padded[0][13])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFEmcwBjL4e_"
      },
      "source": [
        "### Reduced Vocabulary\n",
        "We also want to be able to limit the vocabulary size. Since our padding function produces fixed-length sequences in a numpy matrix, we can use clever numpy indexing to efficiently replace all token ids larger than some value with the designated out-of-vocabulary (OOV) id.\n",
        "\n",
        "In the code below, as an example, we'll keep just token ids less than 1000, replacing all others with OOV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21qpyEgGNQeB"
      },
      "source": [
        "def limit_vocab(sequences, max_token_id, oov_id=2):\n",
        "  \"\"\"Replace token ids greater than or equal to max_token_id with the oov_id.\"\"\"\n",
        "  reduced_sequences = np.copy(sequences)\n",
        "  reduced_sequences[reduced_sequences >= max_token_id] = oov_id\n",
        "  return reduced_sequences\n",
        "\n",
        "# Reduce vocabulary to 1000 tokens.\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
        "print(X_train_reduced[0])\n",
        "\n",
        "# Decode to see what this looks like in tokens. Note the '#' for OOVs.\n",
        "print(decode(X_train_reduced[0]))\n",
        "print(len(X_train_reduced[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d24mOPC6ybC4"
      },
      "source": [
        "### One-hot Encoding\n",
        "Our current feature representations are **sparse**. That is, we only keep track of the token ids that are present in the input. A **one-hot** encoding replaces a value like 22 (corresponding to 'film') with an array with a single 1 at position 22 and zeros everywhere else. This will be very memory-inefficient, but we'll do it anyway for clarity.\n",
        "\n",
        "As discussed above, let's dramatically reduce both the number of token positions (review length) and the number of token ids (vocabulary). We'll clip each review after 20 tokens (so 2500 -> 20) and keep only the most common 1000 tokens (so 90000 -> 1000)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXzkqVL3Jufj"
      },
      "source": [
        "# Keras has a util to create one-hot encodings.\n",
        "X_train_padded = pad_data(X_train, max_length=20)\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
        "X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n",
        "print('X_train_one_hot shape:', X_train_one_hot.shape)\n",
        "print('Print one example', X_train_one_hot[0])\n",
        "print(sum(X_train_one_hot[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_one_hot[0][0])\n",
        "print(\"It will print just 1, since only one 1 in the embedding\")\n",
        "print(sum(X_train_one_hot[0][0]))"
      ],
      "metadata": {
        "id": "aIInoONSsdZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5RvIN4w66Ej"
      },
      "source": [
        "Note the shape of the one-hot encoded features. For each of our 25000 training examples, we have a 20 x 1000 matrix. That is, for each of 20 token positions, we have a vector of 1000 elements containing a single 1 and 999 zeros.\n",
        "\n",
        "We can think of these 1000-dimensional one-hot arrays as **embeddings**. Each token in the input has a 1000-dimensional representation. But because of the one-hot setup, the distance between each pair of tokens is the same ([1,0,0,...], [0,1,0,...], etc.). By contrast, learned embeddings result in meaningful distances between pairs of tokens. We'll get to that soon."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "296Cnt647b5c"
      },
      "source": [
        "## Logistic Regression with One-Hot Encodings\n",
        "Let's start with something familiar -- logistic regression. Since our feature representation is in 2 dimensions (20 x 1000), we need to flatten it to pass it to Keras (remember we did this with the pixel data too). Let's try two strategies for flattening.\n",
        "\n",
        "1. Flatten by *concatenating* (as we did with pixels), turning (20 x 1000) data into (20000,) data. The result is a separate feature for each token at each position.\n",
        "2. Flatten by *averaging* over token positions, turning (20 x 1000) data into (1000,) data. The result is an array with average token counts, ignoring position.\n",
        "\n",
        "NOTE: Our prior assignments have used the standard Stochastic Gradient Descent (SGD) optimizer to compute the gradient from an estimate of the loss (based on the current mini-batch). There are many alternative optimizers. Here we'll use the **Adam** optimizer, which sometimes gives better results. One key characteristic of Adam is that it effectively uses a different learning rate for each parameter rather than a fixed learning rate as in SGD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6m6eebM-0dUW"
      },
      "source": [
        "def build_onehot_model(average_over_positions=False):\n",
        "  \"\"\"Build a tf.keras model for one-hot data.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  if average_over_positions:\n",
        "    # This layer averages over the first dimension of the input by default.\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  else:\n",
        "    # Concatenate.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation=\"sigmoid\"         # sigmoid activation for classification\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy',   # this is a classification task\n",
        "                optimizer='adam',             # fancy optimizer\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NY3W_1-OSZ2X"
      },
      "source": [
        "Now let's try fitting the model to our training data and check performance metrics on the validation (held-out) data. But first, here's a function for plotting the learning curves given the training history object we get from Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOVmajSuMjN6"
      },
      "source": [
        "def plot_history(history):\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
        "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
        "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyE4PgX70_op"
      },
      "source": [
        "model = build_onehot_model()\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train_one_hot,  # one-hot training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuCh9aQPv7F_"
      },
      "source": [
        "### Exercise 2: Comparing logistic regrerssion models (20 points)\n",
        "Train the one-hot model using both the concatenating and the averaging strategies and compare the results. Let's call these *LR-C* (Logistic Regression Concatenating) and *LR-A* (Logistic Regression Averaging). Then answer the following questions:\n",
        "\n",
        "1. What are the final training and validation accuracies for LR-C and LR-A?\n",
        "2. How many parameters are there in each model?\n",
        "3. Would you say that either model is overfitting? Why or why not?\n",
        "4. Briefly describe how LR-C differs from LR-A. How do you explain the relationship between their respective validation accuracy results? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEAN5BejHc__"
      },
      "source": [
        "#### Student Solution\n",
        "1.-\n",
        "\n",
        "|Strats|Val accuracy|Training accuracy|\n",
        "|-|-|-|\n",
        "|LR-C|0.6816|0.81|\n",
        "|LR-A|0.6884|0.6942|\n",
        "\n",
        "2.- In the LR-C model the features are concatenated. So instead of a (20,1000) matrix, it is transformed to a single (20000,) vector. \n",
        "With the LR-A model, the features are average,so instead of a (20,1000) matrix, it's transformed into a vector (1000).\n",
        "When doing model.summary() the number of parameters in the LR-C model is 20001, and the number of parameters fir the LR-A model is 1001.\n",
        "\n",
        "3.- The LR-C (concatenating) is actually overfitting, not as like in the LR-A (averaging).\n",
        "\n",
        "Why? Note that the accuracy is doing very good with the training data, but with the validation data is actually very different since it's performing poorly.\n",
        "The LR-A (averaging) is performing the same with the validation data or with the training data, so it's NOT overfitting.\n",
        "\n",
        "4.- We had different results from the two models. Note that even though LR-C provides better accuracy regarding the training data, the performace is farily poorly comparing to the latter. This would mean that the model is actually overfitting.\n",
        "\n",
        "Note that with the LR-A technique the accuracy is not as a good with the training data than with the LR-C technique, but there are no problems of overfitting.\n",
        "For now, LR-C is better, but if the number of epochs was incremented then this would lead to better results with the LR-A, due to not overfitting.\n",
        "I think LR-A is safer to use at the end, aside of avoiding overfitting, it's also memory efficient since LR-C needs to save a lot of information. In this case is 20 times more features than with LR-A.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2\n",
        "\n",
        "# LR-C contatenating\n",
        "model = build_onehot_model(average_over_positions=False)\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train_one_hot,  # one-hot training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "iAMnrn7W6jk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LR-A contate averaging\n",
        "model = build_onehot_model(average_over_positions=True)\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train_one_hot,  # one-hot training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()         # Get Summary of model. Will display number of parameters"
      ],
      "metadata": {
        "id": "Ke5682Xp8J7m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJIBRqK7lsjG"
      },
      "source": [
        "## Logistic Regression with Embeddings\n",
        "Next, let's train model that replaces one-hot representations of each token with learned embeddings.\n",
        "\n",
        "The code below uses a Keras Embedding layer, which expects to receive a sparse (rather than one-hot) representation. That is, it expects a (padded) sequence of token ids; for each id, it looks up the corresponding embedding vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ho6uOeCaBs2e"
      },
      "source": [
        "def build_embeddings_model(average_over_positions=False,\n",
        "                           vocab_size=1000,\n",
        "                           sequence_length=20,\n",
        "                           embedding_dim=2):\n",
        "  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size,\n",
        "      output_dim=embedding_dim,\n",
        "      input_length=sequence_length)\n",
        "  )\n",
        "\n",
        "  if average_over_positions:\n",
        "    # This layer averages over the first dimension of the input by default.\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  else:\n",
        "    # Concatenate.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation='sigmoid'         # apply the sigmoid function!\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', \n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyhoEjAiFSNB"
      },
      "source": [
        "Try training the model as before. We'll use the averaging strategy rather than the concatenating strategy for dealing with the token sequence. That is, we'll look up embedding vectors for each token. Then we'll average them to produce a single vector. Then we'll traing a logistic regression with that vector as input to predict the binary label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYUE5UwkxoU8"
      },
      "source": [
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1000,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=2)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3k__61hFnag"
      },
      "source": [
        "### Exercise 3: Experiments with embeddings (20 points)\n",
        "Train 6 models with embedding sizes in [2,4,8,16,32,64], keeping other settings fixed. Use the averaging strategy rather than the concatenating strategy.\n",
        "\n",
        "1. Construct a table with the training and validation accuracies of each model (after 5 training epochs).\n",
        "2. Compute the number of parameters in each model.\n",
        "3. Do learned embeddings appear to provide improved performance over the one-hot encoding? Why?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7t46ZdX2ofd"
      },
      "source": [
        "#### Student Solution\n",
        "\n",
        "1.- \n",
        "\n",
        "|Embed size|Training Accuracy|Validation Accuracy|\n",
        "|-|-|-|\n",
        "|2|0.7190|0.7100|\n",
        "|4|0.7302|0.7164|\n",
        "|8|0.7413|0.7208|\n",
        "|16|0.7484|0.7168|\n",
        "|32|0.7520|0.7300|\n",
        "|64|0.7551|0.7284|\n",
        "\n",
        "2.-\n",
        "\n",
        "To get this values I did model.summary() to see how many parameters were being used for each model. Note that since the LR-A technique is being used than to compute this values would be something like this: embedding_size * vocab_size. In this case the vocab_size is always 1000. So the number of parameters would be embedding_size * vocab_size, also remember to add the parameters for the dense layer, which varies. The dense layer has embedding_size + 1 number of parameters.\n",
        "\n",
        "|Embed size|Num parameters |\n",
        "|-|-|\n",
        "|2|2003|\n",
        "|4|4005|\n",
        "|8|8009|\n",
        "|13|16017|\n",
        "|32|32033|\n",
        "|64|64065|\n",
        "\n",
        " 3.- With the one-hot encoding LR-C we had better accuracy 0.8 with the training data, but with the validation data the accuracy was 0.6, note that this was overfitting. With the LR-A the accuracy was lower in both datasets, with 0.6. Note that with embeddings, as the embedding size is continously increasing the accuracies are getting better and better, on both the training and validation data.\n",
        "So at the end embeddings provide better performance than with the one-hot encoding representations. \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traning with embedding size of 2\n",
        "\n",
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1000,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=2)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "7FoT1-FdAPYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traning with embedding size of 4\n",
        "\n",
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1000,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=4)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "hHlmnpDoT8_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traning with embedding size of 8\n",
        "\n",
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1000,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=8)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "xoRbLwS7UvtJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traning with embedding size of 16\n",
        "\n",
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1000,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=16)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ytO6ZZWtVSJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traning with embedding size of 32\n",
        "\n",
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1000,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=32)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "vpcPVzyQVxpM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Traning with embedding size of 64\n",
        "\n",
        "model = build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1000,\n",
        "                               sequence_length=20,\n",
        "                               embedding_dim=64)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "xXcEbHhPWROU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2dWOuxqKHA6"
      },
      "source": [
        "## Inspecting Learned Embeddings\n",
        "Let's retrieve the learned embedding parameters from the trained model and plot the token embeddings.\n",
        "\n",
        "The model layers in a Keras Sequential model are stored as a list and the embeddings are the first layer. We can use the get_weights() function to get a numpy array with the parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfsbGSwkaFjo"
      },
      "source": [
        "# Display the model layers.\n",
        "display(model.layers)\n",
        "\n",
        "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
        "embeddings = model.layers[0].get_weights()[0]\n",
        "display(embeddings.shape)\n",
        "embeddings[0][:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apPWscNwcXTE"
      },
      "source": [
        "Now we'll use a fancy plotting tool called *plotly* to show the embeddings with hovertext so you can move your mouse over the points to see the corresponding tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RZMTrA0KttL"
      },
      "source": [
        "def plot_2d_embeddings(embeddings, id_start=1, count=100):\n",
        "  # Get 1st and 2nd embedding dims for the desired tokens.\n",
        "  x1 = embeddings[id_start:id_start+count, 0]\n",
        "  x2 = embeddings[id_start:id_start+count, 1]\n",
        "\n",
        "  print('x1 shape', x1.shape)\n",
        "  print('x2 shape', x2.shape)\n",
        "  \n",
        "  # Get the corresponding words from the reverse index (for labeling).\n",
        "  tokens = [reverse_index[i] for i in range(id_start, id_start+count)]\n",
        "\n",
        "  # Plot with the plotly library.\n",
        "  data = plotly.Scatter(x=x1, y=x2, text=tokens,\n",
        "                        mode='markers', textposition='bottom left',\n",
        "                        hoverinfo='text')\n",
        "  fig = plotly.Figure(data=[data],\n",
        "                      layout=plotly.Layout(title=\"Word Embeddings\",\n",
        "                                           hovermode='closest'))\n",
        "  fig.show()\n",
        "\n",
        "# Very frequent tokens tend to be more syntactic than semantic, so let's plot\n",
        "# some rarer words.    \n",
        "plot_2d_embeddings(embeddings, id_start=500, count=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3Mm8MjRcZ20"
      },
      "source": [
        "### Exercise 4: Interpretting Embeddings (20 points)\n",
        "Notice that the 2-D embeddings fall in a narrow diagonal band.\n",
        "\n",
        "1. Have the learned embeddings separated positive and negative words? What is the most negative word? Does this make sense?\n",
        "2. Give 2 examples of words that seem to have surprising embedding values and try to explain their positions. For example, what's going on with the tokens '7', '8', and '9'?\n",
        "3. The embedding for 'crazy' is very close to (0,0). Explain what this means in terms of the model's output.\n",
        "4. Can you explain what you think the 2 learned embedding dimensions mean, if anything?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_qAAvvo2y3t"
      },
      "source": [
        "#### Student Solution\n",
        "\n",
        "1. Yes!!!!!! Wow!!!!!!\n",
        "\n",
        "It separates them. You can see for example that words that are on right edge of the diognal band (2-D embeddings) are positive words, meaning that are the most likely to appear on positive reviews. On the other hand, we can see that on the left edge of the diagonal, are negative words, meaning that are the most likely to appear on negative reviews.\n",
        "\n",
        "Interestinly the most negative word is \"avoid\", which makes sense, if the review refers to avoid than it's most likely for this review to be negative. This makes sense.\n",
        "\n",
        "2. The numbers are very weird to see as the most positives, I thought it would've been more. Maybe if the review is positive they tend to give more specific details to things, using values like those. Maybe it's for someone in a review saying something like \"I've watched this movie 7 times, I love it!!\"\n",
        "\n",
        "1st surprising word: Weird. The word weird was actually towards an embedding value which is more positive. I thought it would have been more negative. Maybe this is a description of a positive review, since with a negative review people would have written most harmful words, more hateful words.\n",
        "\n",
        "2nd surprising word: Complete. The word complete was actually towards an embedding value which is more negative. Which was surprising for me. Maybe it's more likely to appear in reviews like \"Not a very complete movie\". Maybe on positive reviews there are more \"better\" words.\n",
        "\n",
        "3. This would mean this is the most complicated word for the model to decide if it's a negative or positive word. The number of positive reviews and negative reviews, containing this word, should be farily the same.\n",
        "\n",
        "4. My initial thought is that it is two tokens, and each other token has the probability of relating to that of those two dimensional tokens. This is why we can plot this values. Maybe that's why we have the numbers as well??\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXCitmUvxfwb"
      },
      "source": [
        "## Scaling Up!\n",
        "Remember how we limited our input sequences to 20 tokens and 1000 vocabulary entries? Let's see how well we can do using more data and bigger models (more parameters)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKZDEGS7xzr6"
      },
      "source": [
        "### Exercise 5: Improve Results (20 points)\n",
        "Using pieces of code from above, set up and train a model that improves the validation accuracy to at least 80%. You should include the following elements:\n",
        "\n",
        "1. Truncate and pad input to the desired length.\n",
        "2. Limit vocabulary to the desired size.\n",
        "3. Set up a model using embeddings.\n",
        "4. Add an additional layer or layers (after the embeddings layer and before the output layer).\n",
        "5. Evaluate on the test data. Remember to apply the same pre-processing to the test data. You can use model.evaluate()."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03xe8sRb2cCy"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def new_build_embeddings_model(average_over_positions=False,\n",
        "                           vocab_size=1000,\n",
        "                           sequence_length=20,\n",
        "                           embedding_dim=2):\n",
        "  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
        "  # Clear session and remove randomness.\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(tf.keras.layers.Embedding(\n",
        "      input_dim=vocab_size,\n",
        "      output_dim=embedding_dim,\n",
        "      input_length=sequence_length)\n",
        "  )\n",
        "\n",
        "  # Added hidden layer with 64 units and relu as the activation function\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=64,\n",
        "      activation='relu'\n",
        "  ))\n",
        "\n",
        "  # Added hidden layer \n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=128,\n",
        "      activation='relu'\n",
        "  ))\n",
        "\n",
        "  # Added another layer\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=128,\n",
        "      activation='relu'\n",
        "  ))\n",
        "\n",
        "  if average_over_positions:\n",
        "    # This layer averages over the first dimension of the input by default.\n",
        "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
        "  else:\n",
        "    # Concatenate.\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      activation='sigmoid'         # apply the sigmoid function!\n",
        "  ))\n",
        "\n",
        "  model.compile(loss='binary_crossentropy', \n",
        "                optimizer='adam',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "DpIICauoRi1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ekbJ4sIq2hID"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "# Trying to get more than 80% accuracy\n",
        "\n",
        "# Modified X_train to have a padding of 38 and a max number of 1500 for our vocabulary\n",
        "X_train_padded = pad_data(X_train, max_length=38)\n",
        "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1500)\n",
        "\n",
        "model = new_build_embeddings_model(average_over_positions=True,\n",
        "                               vocab_size=1500,\n",
        "                               sequence_length=38,\n",
        "                               embedding_dim=128)\n",
        "history = model.fit(\n",
        "  x = X_train_reduced,  # our sparse padded training data\n",
        "  y = Y_train,          # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "history = pd.DataFrame(history.history)\n",
        "plot_history(history)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **Conclusion**\n",
        "\n",
        "For this code I modified the already given code for building a embedding model. But in this case I added three more hidden layers. The first hidden layer had 64 units, the second had 128 units and the third layer had 128 units, all with RELU as the activation function.\n",
        "\n",
        "In addition to this the Training data was padded to have a max length of 38 instead of 20, as well as having a limit number of vocabulary words to be 1500 instead of 1000. \n",
        "\n",
        "A lot of parameters were used 225117, to be exact but luckily I was able to get an accuracy of 0.8051 which is greater than 0.8.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ByY9YAMRd4hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_padded = pad_data(X_test, max_length=38)\n",
        "model.evaluate(X_test_padded, Y_test)"
      ],
      "metadata": {
        "id": "sosjvXAcei9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluated the model with the test data, which was padded to the same 38 tokens as before.\n",
        "\n",
        "Used model.evaluate() to see the results. Where the accuracy is 0.7783 which is worse than our previous 0.8051 of accuracy on the trainind data. It's worse but I think not to be alarmed, and think the model is overfitting."
      ],
      "metadata": {
        "id": "H0fNOj4IgrzN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Instructions\n",
        "\n",
        "1. File > Download .ipynb\n",
        "2. Go to Blackboard, find the submission page, and upload the .ipynb file you just downloaded."
      ],
      "metadata": {
        "id": "Tn_nLbelRkNS"
      }
    }
  ]
}
