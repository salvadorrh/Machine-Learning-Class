{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [Spring 2022] CS 4361 / 5361 - Logistic Regression and Neural Networks",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Before you start**\n",
        "\n",
        "Make a copy of this Colab by clicking on File > Save a Copy in Drive\n",
        "\n",
        "After making a copy, add your student id, last name, and first name to the title."
      ],
      "metadata": {
        "id": "IBzf9VHDzp9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_name = \"Salvador Robles Herrera\"\n",
        "student_id = \"80683116\""
      ],
      "metadata": {
        "id": "RAZs8It7zr2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKsRDH5ZUdfasdv"
      },
      "source": [
        "# Classifying Images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43534tdfgs-v"
      },
      "source": [
        "This exercise is about classifying images - just like Lab 1. Each input is a grid of pixel values and each output is a label from a small set of labels. You will start with a simple binary classifier and finish with a multi-class one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gKL1mrkyLux"
      },
      "source": [
        "### Grading\n",
        "\n",
        "This assignment is worth a total of 100 points."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns  # for nicer plots\n",
        "sns.set(style='darkgrid')  # default style\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "## Understanding the data\n",
        "We'll be using the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. This consists of 70,000 grayscale images (28x28). Each image is associated with 1 of 10 classes. The dataset was split by the creators; there are 60,000 training images and 10,000 test images. \n",
        "\n",
        "Fashion MNIST classes:\n",
        "* T-shirt/top\n",
        "* Trouser\n",
        "* Pullover\n",
        "* Dress\n",
        "* Coat\n",
        "* Sandal\n",
        "* Shirt\n",
        "* Sneaker\n",
        "* Bag\n",
        "* Ankle boot\n",
        "\n",
        "Before doing anything, let's make sure we understand what we're working with. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "load_auto_data_set_text"
      },
      "source": [
        "### Load the data\n",
        "Tensorflow includes a growing [library of datasets](https://www.tensorflow.org/datasets/catalog/overview) and makes it easy to load them in numpy arrays."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "load_auto_data_set_code"
      },
      "source": [
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "# Load the Fashion MNIST dataset.\n",
        "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
        "#print(X_train[0])\n",
        "print(np.shape(Y_train))\n",
        "# Flatten Y_train and Y_test, so they become vectors of label values.\n",
        "# The label for X_train[0] is in Y_train[0].\n",
        "Y_train = Y_train.flatten()\n",
        "Y_test = Y_test.flatten()\n",
        "print(np.shape(Y_train))\n",
        "#print(X_train[0])\n",
        "np.random.seed(0) # For reproducibility purposes\n",
        "\n",
        "# Shuffle the order of the training examples.\n",
        "indices = np.arange(X_train.shape[0])\n",
        "shuffled_indices = np.random.permutation(indices)\n",
        "\n",
        "X_train = X_train[shuffled_indices]\n",
        "Y_train = Y_train[shuffled_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg9mUz9zcIil"
      },
      "source": [
        "### Basic Analysis\n",
        "Notice that `X_train`, `Y_train`, `X_test`, and `Y_test` are all numpy arrays. Let's print their shapes to confirm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6jGmsYAdKEK"
      },
      "source": [
        "# Show the data shapes.\n",
        "print('X_train.shape:', X_train.shape)\n",
        "print('Y_train.shape:', Y_train.shape)\n",
        "print('X_test.shape:', X_test.shape)\n",
        "print('Y_test.shape:', Y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDeim44-dler"
      },
      "source": [
        "Notice that there are 60,000 instances in `X_train`. Each of these is a grayscale image represented by an 28-by-28 array of grayscale pixel values between 0 and 255 (the larger the value, the lighter the pixel). Before we continue, let's apply linear scaling to our pixel values, so they all fall between 0 and 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXukkWBWdlGj"
      },
      "source": [
        "# Pixel values range from 0 to 255. To normalize the data, we just need to \n",
        "# divide all values by 255.\n",
        "X_train = X_train / 255\n",
        "X_test = X_test / 255\n",
        "\n",
        "# So pixel values are from 0-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWsOQ3tbCOA7"
      },
      "source": [
        "In class, we talked about features - properties that characterize our data. Here, we treat **every pixel value as a separate feature**, so each input example has 28x28 (784) features!\n",
        "\n",
        "Fashion MNIST images have one of 10 possible labels (shown above). Since the labels are indices 0-9, let's keep a list of (string) names for convenience."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZpFxxStzSrP"
      },
      "source": [
        "label_names = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
        "               'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
        "\n",
        "# Show the first 5 training labels.\n",
        "print('First 5 label values:', Y_train[0:5])\n",
        "print('Mapped to their names:', [label_names[i] for i in Y_train[0:5]])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5aXeK2VznCW"
      },
      "source": [
        "Next let's use the `imshow` function to look at the first few images in the training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mfV9b4I7cMW"
      },
      "source": [
        "# Create a figure with subplots. This returns a list of object handles in axs\n",
        "# which we can use populate the plots.\n",
        "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(10,5))\n",
        "for i in range(5):\n",
        "  image = X_train[i]\n",
        "  label = Y_train[i]\n",
        "  label_name = label_names[label]\n",
        "  axs[i].imshow(image, cmap='gray')  # imshow renders a 2D grid\n",
        "  axs[i].set_title(label_name)\n",
        "  axs[i].axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNvSx5tybqMR"
      },
      "source": [
        "## Problem Definition: Sneaker Classifier\n",
        "There are many things we can do with this dataset. Following our lectures, let's start with binary classification. We'll use logistic regression to build a sneaker classifier. Our positive examples ($y=1$) will be sneaker images (class 7) and our negative examples ($y=0$) will be all other images.\n",
        "\n",
        "The idea is that our model, given an image $x$, will produce $\\hat{y}$, the probability that $x$ is a sneaker.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E01QY9SRpwPE"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "Recall that logistic regression is an application of the logistic (sigmoid) function to the linear regression model:\n",
        "\n",
        "\\begin{equation}\n",
        "y=\\frac{1}{1+e^{-z}} \n",
        "\\end{equation}\n",
        "\n",
        "<center>\n",
        "<img src=\"https://developers.google.com/machine-learning/crash-course/images/SigmoidFunction.png\" alt=\"Sigmoid Function\" width=\"45%\"/>\n",
        "</center>\n",
        "\n",
        "where:\n",
        "\n",
        "\\begin{equation}\n",
        "z= b + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
        "\\end{equation}\n",
        "\n",
        "In order to map a logistic regression value to a label, a classification threshold needs to be defined. A value above or equal to that threshold indicates that the input should be classified as positive (e.g., sneaker); a value below the threshold indicates that the input should be classified as negative (e.g., non-sneaker).  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Log Loss\n",
        "\n",
        "The loss function for logistic regression is Log Loss (also known as binary cross-entropy), which is defined as follows: \n",
        "\n",
        "\\begin{equation}\n",
        "-\\frac{1}{|Y|} \\sum_{y_i \\in Y}y_i log(\\hat{y}_i) + (1−y_i)log(1−\\hat{y}_i)\n",
        "\\end{equation}\n",
        "\n",
        "Remember that $y$ is either 0 or 1, so either the left term or the right term is active for each example.\n",
        "\n",
        "Log loss is differentiable and convex, and as we have seen, the gradient turns out to be identical to that of MSE in linear regression."
      ],
      "metadata": {
        "id": "487SVTAn9fHl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvcJJ_rUifF2"
      },
      "source": [
        "### Data Preprocessing\n",
        "Before we continue, we need to prepare the data for this binary classification task. We'll map sneaker image labels to 1 (the positive class) and all other images to 0 (the negative class).\n",
        "\n",
        "Programming note: Numpy allows us to perform what is called [boolean array indexing](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#boolean-array-indexing)\n",
        "This means that a numpy array $A$ can be indexed using a boolean array $B$\n",
        "as follows: $A[B]$.\n",
        "We'll take advantage of this type of indexing to prepare the labels for our binary classification task below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3webN4USifuB"
      },
      "source": [
        "# Make copies of the original labels.\n",
        "Y_train_binary = np.copy(Y_train)\n",
        "Y_test_binary = np.copy(Y_test)\n",
        "\n",
        "# Update labels: 1 for sneaker images; 0 for the rest.\n",
        "# Note that a boolean array is created when Y_train_binary != 7 is evaluated.\n",
        "Y_train_binary[Y_train_binary != 7] = 0.0 \n",
        "Y_train_binary[Y_train_binary == 7] = 1.0\n",
        "Y_test_binary[Y_test_binary != 7] = 0.0\n",
        "Y_test_binary[Y_test_binary == 7] = 1.0\n",
        "# Cool, right?\n",
        "\n",
        "# yes!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGgQ-ASYiHbK"
      },
      "source": [
        "### Baseline\n",
        "\n",
        "When dealing with classification problems, a simple, but useful baseline is to select the *majority* class (the most common label in the training set) and use it as the prediction for all inputs.\n",
        "\n",
        "Our training dataset consists of 6,000 sneaker examples (10%), and 54,000 non-sneaker images (90%), so our majority class baseline classifies everything as *non-sneaker*, and will have an accuracy of 90%. We will see if we can beat this baseline with \"Logistic Regression\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls30LfwsIZCv"
      },
      "source": [
        "print('Number of sneaker images in training set:', (Y_train_binary == 1).sum())\n",
        "print('Number of non-sneaker images in training set:', (Y_train_binary == 0).sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 1 (25 points)\n",
        "\n",
        "Implement a function that computes the Log Loss (binary cross-entropy) metric and use it to evaluate our baseline on both the train and test data. Use 0.1 as the predicted probability for your baseline (reflecting what we know about the original distribution of classes in our dataset)."
      ],
      "metadata": {
        "id": "Im_2S20J81PV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# THIS IS JUST FOR TESTING\n",
        "\n",
        "print(\"hello\")\n",
        "a = np.array([0.1,0.9])\n",
        "actual = np.array([0,1])\n",
        "print(a)\n",
        "\n",
        "b = (actual) * np.log(a) + (1-actual) * np.log(1-a)\n",
        "\n",
        "c = [0,0]\n",
        "c[0] = 0 * np.log(0.1) + 1 * np.log(1-0.1)\n",
        "c[1] = 1 * np.log(0.9) + 0 * np.log(1-0.9)\n",
        "\n",
        "print(b)\n",
        "print(c)"
      ],
      "metadata": {
        "id": "UonyobCIL9eb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_loss(Y_true, Y_pred):\n",
        "  \"\"\"Returns the binary log loss for a list of labels and predictions.\n",
        "\n",
        "  Args:\n",
        "    Y_true: A list of (true) labels (0 or 1)\n",
        "    Y_pred: A list of corresponding predicted probabilities\n",
        "\n",
        "  Returns:\n",
        "    Binary log loss\n",
        "  \"\"\"\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  losses = -((Y_true) * np.log(Y_pred) + (1-Y_true) * np.log(1-Y_pred))\n",
        "  return np.sum(losses) / np.shape(Y_true)[0]"
      ],
      "metadata": {
        "id": "gXXYAkch85MN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate baseline on both the train and test data. Use 0.1 as the\n",
        "# predicted probability for your baseline \n",
        "\n",
        "# Note that the baseline is 0.1 since there are 10% of sneakers in the datasets\n",
        "\n",
        "baseline_train = np.full(np.shape(Y_train_binary), 0.1)\n",
        "baseline_test = np.full(np.shape(Y_test_binary), 0.1)\n",
        "\n",
        "# Training data\n",
        "loss_train = log_loss(Y_train_binary, baseline_train)\n",
        "print('Loss train set: ', loss_train)\n",
        "\n",
        "# Testing data\n",
        "loss_test = log_loss(Y_test_binary, baseline_test)\n",
        "print('Loss test set: ', loss_test)"
      ],
      "metadata": {
        "id": "6Hvql8IW_QqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Logistic Regression Classification\n",
        "Now that we have our data ready, let's build our binary classifier! \n",
        "\n",
        "Once again, we will use Tensorflow/Keras to build our logistic regression model. "
      ],
      "metadata": {
        "id": "VJrsee2tRp_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(input_shape, learning_rate=0.1):\n",
        "  \"\"\"Build a TF logistic regression model using Keras.\n",
        "\n",
        "  Args:\n",
        "    input_shape: The shape of the model's input. \n",
        "    learning_rate: The desired learning rate for SGD.\n",
        "\n",
        "  Returns:\n",
        "    model: A tf.keras model (graph).\n",
        "  \"\"\"\n",
        "  # Reset TF and random seed (for reproducible results).\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  # Use the Keras Sequential API as before.\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  # Keras layers can do pre-processing. This layer will take our 28x28 images\n",
        "  # and flatten them into vectors of size 784.\n",
        "  model.add(keras.layers.Flatten(input_shape=input_shape))\n",
        "  \n",
        "  # This layer constructs the linear set of parameters for each input feature\n",
        "  # (as well as a bias), and applies a sigmoid to the result. The result is\n",
        "  # binary logistic regression.\n",
        "  model.add(keras.layers.Dense(\n",
        "      units=1,                     # output dim (for binary classification)\n",
        "      use_bias=True,               # use a bias param\n",
        "      activation='sigmoid'         # apply the sigmoid function!\n",
        "  ))\n",
        "\n",
        "  # We'll use mini-batch SGD and can specify the batch size later.\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "\n",
        "  # Compile the model.\n",
        "  model.compile(loss='binary_crossentropy', \n",
        "                optimizer=optimizer, \n",
        "                metrics=[metrics.binary_accuracy])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "w6z6az-nKfhe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure we can build a model. Before training, the parameters of the model are initialized randomly (this is the default). While the untrained model won't make good predictions, we should still be able to pass data through it and get probability outputs.\n"
      ],
      "metadata": {
        "id": "SygQnB7F9GCU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the model. Notice that we are passing the shape of our images to\n",
        "# the model, as Keras needs to know the dimension of the input.\n",
        "model = build_model(input_shape=X_train[0].shape)\n",
        "\n",
        "# Make a prediction for five inputs.\n",
        "print(model.predict(X_train[0:5]))"
      ],
      "metadata": {
        "id": "aYDm5Rag9IuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the outputs look like probabilities (in [0,1]). Once the model is trained, we hope that these predictions correspond to the probability that each input image is a sneaker.\n"
      ],
      "metadata": {
        "id": "00KXxrMS9Hhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "Let's train the model. Note that we're using 10% of the training data as a *validation split*. This serves a similar purpose to our test data, allowing us to check for over-fitting during training. We don't use the test data here because we might run lots of experiments, and over time, we might adjust settings to improve results on the validation set. We want to preserve the purity of the test data so we can the cleanest possible evaluation at the end of the experimentation process."
      ],
      "metadata": {
        "id": "pW5p-WVl9Mth"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(input_shape=X_train[0].shape, learning_rate=0.1)\n",
        "\n",
        "# Fit the model.\n",
        "history = model.fit(\n",
        "  x = X_train,          # our binary training examples\n",
        "  y = Y_train_binary,   # corresponding binary labels\n",
        "  epochs=5,             # number of passes through the training data\n",
        "  batch_size=64,        # mini-batch size for SGD\n",
        "  validation_split=0.1, # use a fraction of the examples for validation\n",
        "  verbose=1             # display some progress output during training\n",
        "  )\n",
        "\n",
        "# Convert the return value into a DataFrame so we can see the train loss \n",
        "# and binary accuracy after every epoch.\n",
        "history = pd.DataFrame(history.history)\n",
        "display(history)"
      ],
      "metadata": {
        "id": "QfM7eJcQ9N2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train[0].shape)"
      ],
      "metadata": {
        "id": "AaO3mFF1I1YM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good news! Both training accuracy and validation accuracy are near 98\\% (and the loss should be quite a bit lower than your baseline loss calculation above). This means the classifier is quite a bit better than the majority-class baseline.\n"
      ],
      "metadata": {
        "id": "hA5gGe4w9PXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "\n",
        "Let's use the trained model to predict probabilities for the test data. We can use `predict` to run *inference*. The probabilities output by the model will be the probability of the positive class.\n"
      ],
      "metadata": {
        "id": "i-ZN_ByO9Rk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The result of model.predict has an extra dimension, so we flatten to get a\n",
        "# vector of predictions.\n",
        "test_predictions = model.predict(X_test).flatten()\n",
        "print(test_predictions.shape)\n",
        "print(test_predictions)"
      ],
      "metadata": {
        "id": "CgyV4BZ89S-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have probabilities for all images in `X_test`, let's evaluate how well the model performed on our test set. \n"
      ],
      "metadata": {
        "id": "dHMhPUWI9VDV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9hk2vg9A1dm"
      },
      "source": [
        "### Evaluation\n",
        "To turn probabilities into binary predictions, we need to choose a classification threshold. A probability greater than or equal to threshold indicates class *sneaker*, while a value less than the threshold indicates class *non-sneaker*. Once we have binary predictions, we can evaluate performance with some different metrics.\n",
        "\n",
        "A **true positive** is an outcome where the model correctly predicts the positive class. Similarly, a **true negative** is an outcome where the model correctly predicts the negative class.\n",
        "\n",
        "A **false positive** is an outcome where the model incorrectly predicts the positive class. And a **false negative** is an outcome where the model incorrectly predicts the negative class.\n",
        "\n",
        "Using these, we can construct a **confusion matrix**, a table that summarizes performance using these 4 result categories.\n",
        "\n",
        "|                     | Sneaker (Actual)    | Non-Sneaker (Actual) |\n",
        "|---------------------|-----------------|------------------|\n",
        "| **Sneaker (Predicted)**     | Number of True Positives  | Number of False Positives  |\n",
        "| **Non-Sneaker (Predicted)** | Number of False Negatives | Number of True Negatives   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jc9RWUR2I_i_"
      },
      "source": [
        "**Accuracy** is the fraction of examples our model classified correctly (correct / total):\n",
        "\n",
        "\\begin{equation}\n",
        "Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "\\end{equation}\n",
        "\n",
        "**Precision** is the proportion of positive predictions that were actually correct:\n",
        "\n",
        "\\begin{equation}\n",
        "Precision = \\frac{TP}{TP + FP}\n",
        "\\end{equation}\n",
        "\n",
        "**Recall** is the proportion of actual positives that were predicted correctly:\n",
        "\n",
        "\\begin{equation}\n",
        "Recall = \\frac{TP}{TP + FN}\n",
        "\\end{equation}\n",
        "\n",
        "Changing the threshold will change precision and recall, typically with a trade-off: improving precision reduces recall (and vice-versa)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck0yxzxx9Qww"
      },
      "source": [
        "#### Exercise 2 (25 points)\n",
        "\n",
        "\n",
        "Do the following for each of these classification thresholds [0.1, 0.3, 0.5, 0.7, 0.9] using the `test_predictions` (predicted probabilities) and `Y_test_binary` (actual labels):\n",
        "\n",
        "1.   Compute and print counts of [tp, tn, fp, fn], corresponding to the confusion matrix.\n",
        "2.   Compute and print the accuracy, precision, and recall.\n",
        "\n",
        "**Feel free to reuse your solution for Lab 1 and/or use sklearn**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array([0.3,0.5,0.1,0.12])\n",
        "a[a >= 0.2] = 1\n",
        "a[a < 0.2] = 0\n",
        "print(a)\n",
        "c = np.array([0.3,0.5,0.1,0.12])\n",
        "print(c)\n",
        "\n",
        "for i in range(1):\n",
        "  d = np.copy(c)\n",
        "  print(d)\n",
        "\n",
        "print(d)"
      ],
      "metadata": {
        "id": "r3JHBHwops3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(tp,tn,fp,fn):\n",
        "  return (tp+tn)/(tp+tn+fp+fn)\n",
        "\n",
        "def precision(tp,fp):\n",
        "  return (tp)/(tp+fp)\n",
        "\n",
        "def recall(tp,fn):\n",
        "  return (tp)/(tp+fn)"
      ],
      "metadata": {
        "id": "vzo8KmXcvdxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ppmrnh0-jY7-"
      },
      "source": [
        "# Your code goes here\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "thresholds = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
        "\n",
        "for th in thresholds:\n",
        "  print('\\n\\nThreshold: ', th)\n",
        "  pred_binary = np.copy(test_predictions)\n",
        "  pred_binary[pred_binary >= th] = 1\n",
        "  pred_binary[pred_binary < th] = 0\n",
        "  cnf_mtx = confusion_matrix(Y_test_binary, pred_binary)\n",
        "  print(cnf_mtx)\n",
        "  tp = cnf_mtx[0][0]\n",
        "  tn = cnf_mtx[1][1]\n",
        "  fp = cnf_mtx[0][1]\n",
        "  fn = cnf_mtx[1][0]\n",
        "  print('True Positives: ', tp)\n",
        "  print('True Negatives: ', tn)\n",
        "  print('False Positives: ', fp)\n",
        "  print('False Negatives: ', fn)\n",
        "  print('Accuracy: ', accuracy(tp,tn,fp,fn))\n",
        "  print('Precision: ', precision(tp,fp))\n",
        "  print('Recall: ', recall(tp,fn))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiclass Classification\n",
        "\n",
        "We are now ready for the full multi-class case. Instead of using `Y_train_binary` and `Y_test_binary`, we'll use the original labels `Y_train` and `Y_test`. \n"
      ],
      "metadata": {
        "id": "Io_cuRQLQOlW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy Loss\n",
        "\n",
        "Recall the log loss function (above), where $y$ is either 0 or 1:\n",
        "\n",
        "\\begin{equation}\n",
        "-y \\log(\\hat{y}) + (1−y)\\log(1−\\hat{y})\n",
        "\\end{equation}\n",
        "\n",
        "The general form for cross-entropy is used when $y$ is assumed to be a label vector with a 1 in the index of the true label and a 0 everywhere else: $y=[0,0,0,0,0,0,0,1,0,0]$ implies a label of \"sneaker\" in this dataset (the 7th label). Accordingly, $\\hat{y}$ is a vector of predicted probabilities. Then the cross-entropy loss is simply:\n",
        "\n",
        "\\begin{equation}\n",
        "-\\sum_{j} y_j \\log(\\hat{y}_j)\n",
        "\\end{equation}\n",
        "\n",
        "As in the binary case, this summation will have exactly 1 non-zero term where the true label $y_j=1$.\n",
        "\n",
        "Note that this formulation is using a *dense* representation of the label. The corresponding *sparse* representation would use the non-zero index directly ($y=7$)."
      ],
      "metadata": {
        "id": "bpn4J8oF9vly"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2OdfC1Tg41L"
      },
      "source": [
        "#### Exercise 3 (25 points)\n",
        "\n",
        "Fill in the NotImplemented parts of the `build_softmax_model` function below. You will need to make the following changes to generalize the binary case to the multi-class case:\n",
        "* The output will include 10 probabilities instead of 1.\n",
        "* Use a softmax function instead of a sigmoid to normalize output scores to a probability distribution.\n",
        "* Use a [sparse_categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy) loss instead of binary_crossentropy. Note that \"sparse\" refers to the use of a sparse index (e.g. 7) to indicate the label rather than a dense vector (e.g. [0,0,0,0,0,0,0,1,0,0]).\n",
        "\n",
        "Check that training works below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace NotImplemented code below\n",
        "def build_softmax_model(input_shape, n_classes, learning_rate=0.1):\n",
        "  \"\"\"Build a TF logistic regression model using Keras.\n",
        "\n",
        "  Args:\n",
        "    input_shape: The shape of the model's input. \n",
        "    n_classes: Number of classes in dataset\n",
        "    learning_rate: The desired learning rate for SGD.\n",
        "\n",
        "  Returns:\n",
        "    model: A tf.keras model (graph).\n",
        "  \"\"\"\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = keras.Sequential()\n",
        "  model.add(keras.layers.Flatten(input_shape=input_shape))\n",
        "  model.add(keras.layers.Dense(\n",
        "      #units=NotImplemented,\n",
        "      units=10,\n",
        "      #activation=NotImplemented\n",
        "      activation='softmax'\n",
        "  ))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "  model.compile(#loss=NotImplemented,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                optimizer=optimizer, \n",
        "                metrics=['accuracy'])\n",
        "  return model"
      ],
      "metadata": {
        "id": "cochiep7OOsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training \n",
        "\n",
        "Train your model. If implemented correctly, you should reach validation accuracy 0.838 after 5 epochs."
      ],
      "metadata": {
        "id": "KBjwh6WLQmI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_model = build_softmax_model(X_train[0].shape, len(label_names), 0.1)\n",
        "\n",
        "softmax_history = softmax_model.fit(\n",
        "  x = X_train,\n",
        "  y = Y_train,\n",
        "  epochs=5,\n",
        "  batch_size=64,\n",
        "  validation_split=0.1,\n",
        "  verbose=1)\n",
        "\n",
        "softmax_history = pd.DataFrame(softmax_history.history)\n",
        "display(softmax_history)"
      ],
      "metadata": {
        "id": "9RQDVnguPVmB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference\n",
        "\n",
        "As in the binary case, we can use our trained model to predict probabilities for new examples. Now that we have an output distribution over 10 class, the `predict` function will produce the corresponding 10 probabilities. We can use `argmax` to select the index of the highest probability."
      ],
      "metadata": {
        "id": "uH-ztYRQOvyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Recall that model.predict gives a vector of probabilities for each x.\n",
        "# Get labels by taking the argmax -- the index with the largest probability.\n",
        "softmax_test_predictions = np.argmax(softmax_model.predict(X_test), axis=-1)\n",
        "print(softmax_test_predictions)"
      ],
      "metadata": {
        "id": "H-A9Ej7t-GIH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Overall accuracy gives a very limited picture of the model's performance. A confusion matrix gives a much deeper summary."
      ],
      "metadata": {
        "id": "a2T7-uL6-HnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix as a 2D array.\n",
        "softmax_confusion_matrix = tf.math.confusion_matrix(Y_test,\n",
        "                                                    softmax_test_predictions)\n",
        "\n",
        "# Use a heatmap plot to display it.\n",
        "ax = sns.heatmap(softmax_confusion_matrix, annot=True, fmt='.3g', cmap='Blues',\n",
        "                 xticklabels=label_names, yticklabels=label_names, cbar=False)\n",
        "\n",
        "# Add axis labels.\n",
        "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwe9J8FJ-I0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feed Forward Neural Network Classification\n",
        "\n",
        "We're now ready to build our first neural network. This is as easy as adding \"hidden\" layers to our model structure."
      ],
      "metadata": {
        "id": "Yb9mapxy-Kkv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise 4 (25 points)\n",
        "\n",
        "Fill in the implementation of the `build_ffnn_softmax_model` function below. This will be almost identical to `build_softmax_model` (above), but should include **2 hidden layers**, each with **128 units**, and each using a **relu activation**."
      ],
      "metadata": {
        "id": "qPJAlv9m-MQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_ffnn_softmax_model(input_shape, n_classes, learning_rate=0.1):\n",
        "  \"\"\"Build a TF feed-forward neural network model using Keras.\n",
        "\n",
        "  Args:\n",
        "    input_shape: The shape of the model's input. \n",
        "    n_classes: Number of classes in dataset\n",
        "    learning_rate: The desired learning rate for SGD.\n",
        "\n",
        "  Returns:\n",
        "    model: A tf.keras model (graph).\n",
        "  \"\"\"\n",
        "  tf.keras.backend.clear_session()\n",
        "  tf.random.set_seed(0)\n",
        "\n",
        "  model = keras.Sequential()\n",
        "\n",
        "  # YOUR CODE HERE\n",
        "  model.add(keras.layers.Flatten(input_shape=input_shape))\n",
        "  # 1st hidden layer\n",
        "  model.add(keras.layers.Dense(\n",
        "      units=128,\n",
        "      activation='relu'\n",
        "  ))\n",
        "  # 2nd hidden layer\n",
        "  model.add(keras.layers.Dense(\n",
        "      units=128,\n",
        "      activation='relu'\n",
        "  ))\n",
        "\n",
        "  # The actual layer from above (not hidden)\n",
        "  model.add(keras.layers.Dense(\n",
        "      units=10,\n",
        "      activation='softmax'\n",
        "  ))\n",
        "\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
        "  model.compile(#loss=NotImplemented,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                optimizer=optimizer, \n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "UiAryW-F-O3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training \n",
        "\n",
        "Train your model. If implemented correctly, you should reach validation accuracy 0.867 after 5 epochs."
      ],
      "metadata": {
        "id": "yYojNqIW-RQP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffnn_softmax_model = build_ffnn_softmax_model(\n",
        "    X_train[0].shape, len(label_names), 0.1)\n",
        "\n",
        "ffnn_softmax_history = ffnn_softmax_model.fit(\n",
        "  x = X_train,\n",
        "  y = Y_train,\n",
        "  epochs=5,\n",
        "  batch_size=64,\n",
        "  validation_split=0.1,\n",
        "  verbose=1)\n",
        "\n",
        "ffnn_softmax_history = pd.DataFrame(ffnn_softmax_history.history)\n",
        "display(ffnn_softmax_history)"
      ],
      "metadata": {
        "id": "S636qN4q-S0l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ffnn_softmax_model.summary()\n"
      ],
      "metadata": {
        "id": "eZ9V_F1Y-UUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation\n",
        "\n",
        "Let's compare the confusion matrices for the linear model (softmax_model) and the neural network model (ffnn_softmax_model) to see which specific confusions are fixed by the improved model."
      ],
      "metadata": {
        "id": "Wz5XkExM-WEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ffnn_softmax_test_predictions = np.argmax(\n",
        "    ffnn_softmax_model.predict(X_test), axis=-1)\n",
        "\n",
        "# Create a confusion matrix as a 2D array.\n",
        "ffnn_softmax_confusion_matrix = tf.math.confusion_matrix(\n",
        "    Y_test, ffnn_softmax_test_predictions)\n",
        "\n",
        "# Use a heatmap plot to display it.\n",
        "diff_confusion_matrix = ffnn_softmax_confusion_matrix - softmax_confusion_matrix\n",
        "ax = sns.heatmap(diff_confusion_matrix, annot=True, fmt='.3g', cmap='Blues',\n",
        "                 xticklabels=label_names, yticklabels=label_names, cbar=False)\n",
        "\n",
        "# Add axis labels.\n",
        "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
        "plt.show()\n",
        "\n",
        "# Remember that this plot is not the plot for the Neural Network model alone"
      ],
      "metadata": {
        "id": "nDV661vg-XbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Exercise 5 (ungraded - 0 points - just for extra fun)\n",
        "\n",
        "a. In this confusion matrix of differences, what do the diagonal values represent, and what do the off-diagonal values represent?\n",
        "\n",
        "b. Which confused pair is most improved by the neural network model?"
      ],
      "metadata": {
        "id": "NeIE2pZ6-apv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "WRITE YOUR ANSWERS HERE\n",
        "\n",
        "a. The diagonal values represent the True positives. Meaning that if we have big numbers in that diagonal then it means that we have a good accuracy, a greater value. If we have more values outside of the diagonal than that means that our model didn't perform too well since we those would be the values that weren't predicted correclty.\n",
        "\n",
        "With the previous model it means that, since there was a subtraction of the new obtained model (using Neural Networks) and the previous model (using Linear Regression), then that would mean that the higher values we have in the diagonal implies that the Neural Network model's accuracy is the better.\n",
        "\n",
        "This is actually what we got, since the values in the diagonal are actually of a relatively high value, meaning that the new model is much better than the previous. \n",
        "\n",
        "b. The Neural Network best improves the accuracy of detecting a pullover, 69 examples of improvement. If I would need to guess which pair of classes, than I would guess that the pullover and the shirt.\n",
        "\n",
        "Why the shirt? Notice that the column with higher negative values is the shirt, meaning that we had less \"false shirts\". Pullover and a shirt are similar images as well, so I think that this model better estimates which is a shirt and which is a pullover."
      ],
      "metadata": {
        "id": "YjCsTvSB-kXo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Instructions\n",
        "\n",
        "1. File > Download .ipynb\n",
        "2. Go to Blackboard, find the submission page, and upload the .ipynb file you just downloaded."
      ],
      "metadata": {
        "id": "HAWUwNgkPcMX"
      }
    }
  ]
}
