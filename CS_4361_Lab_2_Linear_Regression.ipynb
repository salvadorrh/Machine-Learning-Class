{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [Spring 2022] CS 4361 / 5361 - Lab 2 - Linear Regression",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Before you start**\n",
        "\n",
        "Make a copy of this Colab by clicking on File > Save a Copy in Drive\n",
        "\n",
        "After making a copy, add your student id, last name, and first name to the title."
      ],
      "metadata": {
        "id": "IuQ0NbnMx40_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_name = \"Salvador Robles Herrera\"\n",
        "student_id = \"80683116\""
      ],
      "metadata": {
        "id": "nvu67cFPx63V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKsRDH5ZUdfasdv"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43534tdfgs-v"
      },
      "source": [
        "This assignment introduces linear regression with gradient descent, but starts with basic code building blocks.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "overview"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rY7emKinFRCr"
      },
      "source": [
        "###Learning Objectives\n",
        "* Practice manipulating numpy arrays.\n",
        "* Generate artifical data, which can be useful for building intuition and debugging issues with complex models.\n",
        "* Use Mean Squared Error to compare models.\n",
        "* Understand generalization and overfitting.\n",
        "* Work through the derivation of gradient descent for linear regression.\n",
        "* Apply gradient descent to build intuition about training models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gKL1mrkyLux"
      },
      "source": [
        "### Grading\n",
        "\n",
        "This lab (exercises 1-8) is worth a total of 100 points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHLcriKWLRe4"
      },
      "source": [
        "## Artificial Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxDM3E2icDIg"
      },
      "source": [
        "As a warm-up, let's get some practice manipulating matrices with numpy, plotting with matplotlib, and computing evaluation metrics for some simple models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7X58hOMTUH-w"
      },
      "source": [
        "# Import the libraries we'll use below.\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print these to make sure you understand what is being generated.\n",
        "A = np.array([1, 2, 3])\n",
        "B = np.arange(1, 13).reshape(3, 4)\n",
        "C = np.ones((2, 3))\n",
        "D = np.eye(3)\n",
        "\n",
        "print('A')\n",
        "print(A)\n",
        "print('B')\n",
        "print(B)\n",
        "print('C')\n",
        "print(C)\n",
        "print('D')\n",
        "print(D)"
      ],
      "metadata": {
        "id": "46czllfgkfCR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4wvvzKoUIAN"
      },
      "source": [
        "### Exercise 1: Matrix manipulation (17.5pts)\n",
        "\n",
        "Perform the following computations using numpy functions and print the results. Each can easily be expressed in a single line of code.\n",
        "\n",
        "Hint: One of the below computations will fail, the error explains why it failed - identify a way to fix it. \n",
        "\n",
        "1. 2A + 1\n",
        "2. Sum the rows of B\n",
        "3. Sum the columns of B\n",
        "4. Number of elements of B greater than 5\n",
        "5. C + C\n",
        "6. A * B\n",
        "7. (B * B) - D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiJInaRk6TbO"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJtwrjdO6TbS"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "# Exercise 1, A -> [1,2,3]\n",
        "A1 = 2*A + 1\n",
        "print('Sub exercise 1: \\n', A1)\n",
        "\n",
        "# Exercise 2, rows sum\n",
        "B1 = B.sum(axis=1)\n",
        "print('Sub exercise 2: \\n', B1)\n",
        "\n",
        "# Exercise 3, cols sum\n",
        "B2 = B.sum(axis=0)\n",
        "print('Sub exercise 3: \\n', B2)\n",
        "\n",
        "# Exercise 4, Num elements > 5\n",
        "B3 = np.sum(B > 5)\n",
        "print('Sub exercise 4: \\n', B3)\n",
        "\n",
        "# Exercise 5, C + C\n",
        "C1 = C + C\n",
        "print('Sub exercise 5: \\n', C1)\n",
        "\n",
        "# Exercise 6, A * B\n",
        "A2 = np.dot(A, B)\n",
        "print('Sub exercise 6: \\n', A2)\n",
        "\n",
        "# Exercise 7, (B*B) - D\n",
        "#B4 = (B * B) - D\n",
        "B4 = (B * B) - np.append(D, np.zeros((np.shape(D)[1],1)), axis = 1)\n",
        "print('Sub exercise 7\\n', B4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbCRG2-uUKCT"
      },
      "source": [
        "## Data for Supervised Learning\n",
        "Supervised learning is all about learning to make predictions: given an input $x$ (e.g. home square footage), can we predict an output $\\hat{y}$ (e.g. estimated value) as close to the actual observed output $y$ (e.g. sale price) as possible. Note that the \"hat\" above $y$ is used to denote an estimated or predicted value. The *output* is sometimes referred to as a *target* or a *label*.\n",
        "\n",
        "Let's start by generating some artificial data. We'll create a vector of inputs, $X$, and a corresponding vector of target outputs $Y$. In general, we'll refer to invidual examples with a lowercase ($x$), and a vector or matrix containing multiple examples with a capital ($X$)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ulmn_bFdU87t"
      },
      "source": [
        "def create_1d_data(num_examples=10, w=2, b=1, random_scale=1):\n",
        "  \"\"\"Create X, Y data with a linear relationship with added noise.\n",
        "\n",
        "  Args:\n",
        "    num_examples: number of examples to generate\n",
        "    w: desired slope\n",
        "    b: desired intercept\n",
        "    random_scale: add uniform noise between -random_scale and +random_scale\n",
        "\n",
        "  Returns:\n",
        "    X and Y with shape (num_examples)\n",
        "  \"\"\"\n",
        "  X = np.arange(num_examples)\n",
        "  np.random.seed(4)  # consistent random number generation\n",
        "  noise = np.random.uniform(low=-random_scale, high=random_scale, size=X.shape)\n",
        "  Y = w * X + b + noise\n",
        "  return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6qJg0IiYVJ8U"
      },
      "source": [
        "# Create some artificial data using create_1d_data.\n",
        "X, Y = create_1d_data()\n",
        "plt.scatter(X, Y)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6coKbXSpXOz"
      },
      "source": [
        "### Exercise 2: Models for Data (7.5 pts)\n",
        "A model is a function that takes an input $x$ and produces a prediction $\\hat{y}$.\n",
        "\n",
        "Let's consider two possible models for this data:\n",
        "1. $M_1(x)  = x+5$ \n",
        "2. $M_2(x) = 2x+1$\n",
        "\n",
        "Compute the predictions of models $M_1$ and $M_2$ for the values in $X$. These predictions should be vectors of the same shape as $Y$. Then plot the prediction lines of these two models overlayed on the \"observed\" data $(X, Y)$. Use [plt.plot()](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html) to draw the lines."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-z7qTVRJo5B"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3XjCKbZQBBw7"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "M1_Y = X + 5\n",
        "M2_Y = (2 * X) + 1 \n",
        "plt.plot(X, M1_Y, 'g') # Green line is M1 model\n",
        "plt.plot(X, M2_Y, 'r') # Red line is M2 model\n",
        "plt.plot(X, Y) # Blue line is expected model\n",
        "\n",
        "# Note that by just visualizing on the plot, M2 model is closest to the expected\n",
        "# value line."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NH-0soZiWx9x"
      },
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "How good are our models? Intuitively, the better the model, the more closely it fits the data we have. That is, for each $x$, we'll compare $y$, the true value, with $\\hat{y}$, the predicted value. This comparison is often called the *loss* or the *error*. One common such comparison is *squared error*: $(y-\\hat{y})^2$. Averaging over all our data points, we get the *mean squared error*:\n",
        "\n",
        "\\begin{equation}\n",
        "\\textit{MSE} = \\frac{1}{|Y|} \\sum_{y_i \\in Y}(y_i - \\hat{y}_i)^2\n",
        "\\end{equation}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AyY2DpxYLI0"
      },
      "source": [
        "### Exercise 3: Computing MSE (7.5 pts)\n",
        "Write a function for computing the MSE metric and use it to compute the MSE for the two models above, $M_1$ and $M_2$. You should be able to do this **without using a for loop**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmUXw-R9YykQ"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCeAfI5mW9sg"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "def MSE(true_values, predicted_values):\n",
        "  \"\"\"Return the MSE between true_values and predicted values.\"\"\"\n",
        "  error = predicted_values - true_values\n",
        "  sq_error = np.square(error)\n",
        "  return np.sum(sq_error) / np.shape(sq_error)[0] # Average of square errors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmgLEC6L5h4r"
      },
      "source": [
        "print(MSE(Y, M1_Y))\n",
        "print(MSE(Y, M2_Y))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDiy3OZwZlwj"
      },
      "source": [
        "## Generalization\n",
        "\n",
        "Our data $(X, Y)$ represents just a sample of all possible input-output pairs we might care about. A model will be useful to the extent we can apply it to new inputs. Consider the more complex model below, which appears to produce a much smaller mean squared error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns1siZ9DZvSY"
      },
      "source": [
        "# Fit an 8-th degree polynomial to (X, Y). See np.polyfit for details.\n",
        "polynomial_model_coefficients = np.polyfit(X, Y, deg=8)\n",
        "polynomial_model = np.poly1d(polynomial_model_coefficients)\n",
        "M3 = polynomial_model(X)\n",
        "fig = plt.scatter(X, Y)\n",
        "plt.plot(X, M3, '-k')\n",
        "print ('MSE for M3:', MSE(Y, M3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2m9YmLMZ1EV"
      },
      "source": [
        "### Exercise 4: Generalization (10 pts)\n",
        "Apply models M2 and M3 to the test data below. Print the predicted outputs of the models and compare the MSE between the predictions and true values (Y_test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUW1gSlHBxut"
      },
      "source": [
        "# Suppose we have the following test data:\n",
        "X_test = np.array([10, 11, 12])\n",
        "Y_test = np.array([22.1, 23.4, 26.2])\n",
        "\n",
        "# Plot train and test data.\n",
        "plt.scatter(X, Y, color='k', label='Train')\n",
        "plt.scatter(X_test, Y_test, color='r', label='Test')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0Zpx79_aQEC"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWsB_pQRDrk2"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "M3_Y_2 = polynomial_model(X_test)\n",
        "M2_Y_2 = (2 * X_test) + 1\n",
        "plt.plot(X_test, M3_Y_2)\n",
        "plt.plot(X_test, M2_Y_2)\n",
        "print('MSE for M3: ', MSE(M3_Y_2, Y_test))\n",
        "print('MSE for M2: ', MSE(M2_Y_2, Y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCWuhbfLNmna"
      },
      "source": [
        "## Notation\n",
        "In our artificial data, things are pretty simple: each input example is just a single value. But soon, each input example will include multiple values or *features*, so we need some conventions to avoid confusion.\n",
        "\n",
        "Let's start with the inputs:\n",
        "\n",
        "\\begin{align}\n",
        "X =\n",
        "\\begin{pmatrix}\n",
        "x^{(0)} \\\\\n",
        "x^{(1)} \\\\\n",
        "\\vdots \\\\\n",
        "x^{(m-1)}\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "* Capital $X$ refers to all input examples together.\n",
        "* Lowercase $x$ refers to an individual input example; we use $x^{(i)}$ to refer to input example $i$; there are $m$ total examples.\n",
        "\n",
        "Further, each input example $x$ could itself be a vector of feature values:\n",
        "\n",
        "\\begin{align}\n",
        "x = [x_0, x_1, \\dots x_{n-1}]\n",
        "\\end{align}\n",
        "\n",
        "* Lowercase $x$ refers to all input features together for an individual input example.\n",
        "* $x_i$ refers to feature $i$ for an input example $x$; there are $n$ total features.\n",
        "\n",
        "Similarly, we can index labels $y^{(i)}$ in $Y$, which we can think of as a column vector where $y^{(i)}$ is the label for $x^{(i)}$.\n",
        "\n",
        "\\begin{align}\n",
        "Y =\n",
        "\\begin{pmatrix}\n",
        "y^{(0)} \\\\\n",
        "y^{(1)} \\\\\n",
        "\\vdots \\\\\n",
        "y^{(m-1)}\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "In general, we're using matrix notation. Rows refer to examples and columns refer to features. If we want to be very specific and refer to a particular feature of a particular input example, we can use $x_{i,j}$ for input $i$, feature $j$. Using matrices will be useful for coding ML algorithms since most of the operations we will do can be expressed as operations on matrices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2szkkNDvsCfn"
      },
      "source": [
        "##Parameter Vectors\n",
        "Let's prepare to learn a linear model $h(x)$ that approximates values of $Y$ from corresponding values of $X$. Since our input data has only one feature, our model will have two parameters (also called weights), which we'll refer to collectively as $W$:\n",
        "\n",
        "\\begin{align}\n",
        "h(x) = w_0 + w_1x\n",
        "\\end{align}\n",
        "\n",
        "Notice that if we prepend an extra feature (column) to $X$ that is always $1$, we can rewrite our model using a matrix multiplication:\n",
        "\n",
        "\\begin{align}\n",
        "h(x) = w_0x_0 + w_1x_1 = xW^T\n",
        "\\end{align}\n",
        "\n",
        "To make this matrix formulation as clear as possible, this is:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y} = xW^T =\n",
        "\\begin{pmatrix}\n",
        "x_0 & x_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "In addition, if we wanted to apply our model to *all* inputs $X$, we could simply use $XW^T$:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{Y} = XW^T =\n",
        "\\begin{pmatrix}\n",
        "x_{0,0} & x_{0,1} \\\\\n",
        "x_{1,0} & x_{1,1} \\\\\n",
        "\\vdots & \\vdots \\\\\n",
        "x_{m-1,0} & x_{m-1,1} \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0 \\\\\n",
        "w_1 \\\\\n",
        "\\end{pmatrix}\n",
        "\\end{align}\n",
        "\n",
        "Remember that [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) requires the inner dimensions to line up: \n",
        "\n",
        "\\begin{align}\n",
        "X_{\\{m \\times n\\}} W^T_{\\{n \\times 1 \\}}  = \\hat{Y}_{\\{m \\times 1 \\}}\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NXo1n9j1LMT"
      },
      "source": [
        "### Exercise 5: Practice with Parameters (12.5 pts)\n",
        "Add a column of 1s to $X$. Then, use matrix multiplication (np.dot) to apply $M_1$ and $M_2$ (from above) to produce vectors of predictions. Print the shapes of the predictions to validate that they have the same shape as $Y$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0FIiQ-j6tgn"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBEZ_QOX6qOi"
      },
      "source": [
        "# Add a column of 1s to X by using np.c_ to concatenate with the current values.\n",
        "print(X)\n",
        "X_with_1s = np.c_[np.ones(X.shape[0]), X]\n",
        "print(X_with_1s)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "M1 = [5, 1]\n",
        "M2 = [1, 2]\n",
        "\n",
        "pred_M1 = np.dot(X_with_1s, M1)\n",
        "pred_M2 = np.dot(X_with_1s, M2)\n",
        "print('Shape for predictions M1: ', np.shape(pred_M1))\n",
        "print('Shape for predictions M2: ', np.shape(pred_M2))\n",
        "print('Predictions M1: ', pred_M1)\n",
        "print('Predictions M2: ', pred_M2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LUNVK2acFMQ0"
      },
      "source": [
        "## Gradient Descent\n",
        "Here we'll demonstrate gradient descent for linear regression to learn the weight vector $W$. We'll use the more specific notation $h_W(x)$ since we want to specify that $h$ is parameterized by $W$. As above, we'll assume that $x_0=1$ so we can write $h$ as a sum or a matrix product:\n",
        "\n",
        "\\begin{align}\n",
        "h_W(x) = \\sum_{i=0}^{n-1} w_i x_i = x W^T\n",
        "\\end{align}\n",
        "\n",
        "In the derivation that follows, we'll use summations, but in the code below, we'll use matrix computations.\n",
        "\n",
        "In linear regression, we compute the loss, $J(W)$ from the mean squared difference between predictions $h_W(x)$ and targets $y$. In the following equation, we average the loss over each of the $m$ training examples.\n",
        "\n",
        "\\begin{align}\n",
        "J(W) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (h_W(x^{(i)}) - y^{(i)})^2\n",
        "\\end{align}\n",
        "\n",
        "Dividing by $2$ simplifies the formula of the gradient, since it cancels out the constant $2$ from by the derivative of the squared term (see below). Remember that the gradient is a vector of partial derivatives for each $w_j$ (holding the other elements of $w$ constant). The gradient points in direction of steepest ascent for the loss function $J$.\n",
        "\n",
        "Here we derive the parameter update rule by computing the gradient of the loss function. We need a derivative for each feature in $x$, so we'll show how to compute the derivative with respect to $w_j$. For simplicity, let's assume we have only one training example ($m = 1$):\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial}{\\partial w_j} J(W) &= \\frac{\\partial}{\\partial w_j} \\frac{1}{2} (h_W(x) - y)^2 \\tag{1}\\\\\n",
        "&= 2 \\cdot \\frac{1}{2} (h_W(x) - y) \\cdot \\frac{\\partial}{\\partial w_j} (h_W(x) - y) \\tag{2}\\\\\n",
        "&= (h_W(x) - y) \\frac{\\partial}{\\partial w_j} \\left(\\sum_{i=0}^{n-1} w_i x_i - y \\right) \\tag{3}\\\\\n",
        "&= (h_W(x) - y)x_j \\tag{4}\n",
        "\\end{align}\n",
        "\n",
        "The derivation has 2 key steps:\n",
        "\n",
        "(1) Apply the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) (step 1 -> 2).\n",
        "\n",
        "(2) The derivative with respect to $w_j$ of $h_W(x)$ is only non-zero for $w_j x_j$. For this component, the derivative is $x_j$ since the feature value is treated as a constant (step 3 -> 4).\n",
        "\n",
        "Ok, that's it. We can now implement gradient descent for linear regression. The only difference in the code below is that it computes the loss as an average over all training examples (rather than just a single example)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaXYiTm9ftRf"
      },
      "source": [
        "### Exercise 6: Implementing Gradient Descent for Linear Regression (20 pts)\n",
        "Fill in the `NotImplemented` parts of the gradient descent function below. There are detailed comments to help guide you. Note that this function uses vectors and matrices so you'll want to use numpy functions like `np.dot` to multiply them, for example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTlTUJkS4DzQ"
      },
      "source": [
        "##### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hP9rzDyFXTg"
      },
      "source": [
        "def gradient_descent(inputs, outputs, learning_rate, num_epochs):\n",
        "  \"\"\"Apply the gradient descent algorithm to learn learn linear regression.\n",
        "\n",
        "  Args:\n",
        "    inputs: A 2-D array where each column is an input feature and each\n",
        "            row is a training example.\n",
        "    outputs: A 1-D array containing the real-valued\n",
        "             label corresponding to the input data in the same row.\n",
        "    learning_rate: The learning rate to use for updates.\n",
        "    num_epochs: The number of passes through the full training data.\n",
        "\n",
        "  Returns:\n",
        "    weights: A 2-D array with the learned weights after each training epoch.\n",
        "    losses: A 1-D array with the loss after each epoch.\n",
        "  \"\"\"\n",
        "  # m = number of examples, n = number of features\n",
        "  m, n = inputs.shape\n",
        "  \n",
        "  # We'll use a vector of size n to store the learned weights and initialize\n",
        "  # all weights to 1. \n",
        "  W = np.ones(n)\n",
        "  \n",
        "  # Keep track of the training loss and weights after each step.\n",
        "  losses = []\n",
        "  weights = []\n",
        "  \n",
        "  for epoch in range(num_epochs):\n",
        "      # Append the old weights to the weights list to keep track of them.\n",
        "      weights.append(W)\n",
        "\n",
        "      # Evaluate the current predictions for the training examples given\n",
        "      # the current estimate of W (you did this in exercise 5). \n",
        "      # predictions = NotImplemented\n",
        "      predictions = np.dot(inputs, np.transpose(W))\n",
        "      # predictions = np.dot(inputs, W)\n",
        "\n",
        "      # Find the difference between the predictions and the actual targetvalues.\n",
        "      diff = (predictions - outputs) #/ np.shape(inputs)[0]\n",
        "      \n",
        "      # In standard linear regression, we want to minimize the sum of squared\n",
        "      # differences. Find the loss as we did in Excerise 3 .\n",
        "      loss = MSE(predictions, outputs) / 2\n",
        "\n",
        "      # Append the loss to the losses list to keep a track of it.\n",
        "      losses.append(loss)\n",
        "      \n",
        "      # Compute the gradient with respect to the loss.\n",
        "      # [Formula (4) in the Gradient Descent Implementation]\n",
        "      #gradient = NotImplemented\n",
        "      # 4) (hw(x) - y) x = (diff) (inputs)\n",
        "      gradient = np.dot(diff, inputs) / m\n",
        "\n",
        "      # Update weights, scaling the gradient by the learning rate.\n",
        "      W = W - learning_rate * gradient\n",
        "      \n",
        "  return np.array(weights), np.array(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXN_YY-daSPK"
      },
      "source": [
        "Let's try running gradient descent with our artificial data and print out the results. Note that we're passing the version of the input data with a column of $1s$ so that we learn an *intercept* (also called a *bias*). We can also try learning without the intercept.\n",
        "\n",
        "Note: if your implementation of gradient descent is correct, you should get a loss of ~0.20458 after 5 epochs (with a bias parameter)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4z23bKHayGU"
      },
      "source": [
        "print('Running gradient descent...')\n",
        "weights, losses = gradient_descent(X_with_1s, Y, learning_rate=.02,\n",
        "                                   num_epochs=5)\n",
        "for W, loss in zip(weights, losses):\n",
        "  print(loss, W)\n",
        "\n",
        "print('\\nRunning gradient descent without biases...')\n",
        "# Make sure we're providing an input with the right 2-D shape.\n",
        "X_without_1s = np.expand_dims(X, axis=0).T\n",
        "weights_without_bias, losses_without_bias = gradient_descent(X_without_1s, Y,\n",
        "                                                             .02, num_epochs=5)\n",
        "for W, loss in zip(weights_without_bias, losses_without_bias):\n",
        "  print(loss, W)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7vGx68Ec2Si"
      },
      "source": [
        "### Exercise 7: Interpreting the Model (12.5 pts)\n",
        "Write down the learned model with and without an intercept term. Which model fits the data better?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlVWFCRtqZIZ"
      },
      "source": [
        "#### Student Solution\n",
        "\n",
        "WRITE YOUR ANSWERS HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First I can notice that the resulting weights after 5 full passes in the data (num_epochs = 5) are [1.1627, 1.93072] when there are biases (intercepts) and the weights without biases are [2.1037].\n",
        "\n",
        "First Model: f(x1, x2) = 1.1627 * x1 + 1.93 * x2\n",
        "\n",
        "Second Model: f(x1, x2) = x1 + 2.1037 * x2\n",
        "\n",
        "After at looking at the errors the Gradient Descent printed, we can see that the first run was actually better than the second run. How do we know when a learned model is better than the other?\n",
        "\n",
        "We just check the Mean Square Error results, which will indicate how far are the results from being right, when the value of the MSE is higher than that means that we have a very vad model, but if we have a model that has a low MSE than that means that we have a good model, that performs good.\n",
        "\n",
        "Then that meaning that first model fits the data better since we have a lower MSE, than the second model. 0.2045 vs 0.6175 of MSE."
      ],
      "metadata": {
        "id": "L7891I7UgrsQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-DdmWmPrDvk"
      },
      "source": [
        "## Gradient Descent Progress\n",
        "Let's write a function that lets us visualize the progress of gradient descent during training. Our gradient descent function already provides intermediate weight vectors and losses after each epoch, so we just need to plot these."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWcTeR_gNz9d"
      },
      "source": [
        "def plot_learning(inputs, outputs, weights, losses):\n",
        "  \"\"\"Plot predictions and losses after each training epoch.\n",
        "\n",
        "  Args:\n",
        "    inputs: A 2-D array where each column is an input feature and each\n",
        "            row is a training example.\n",
        "    outputs: A 1-D array containing the real-valued\n",
        "             label corresponding to the input data in the same row.\n",
        "    weights: A 2-D array with the learned weights after each training epoch.\n",
        "    losses: A 1-D array with the loss after each epoch.\n",
        "  \"\"\"\n",
        "  # Create a figure.\n",
        "  plt.figure(1, figsize=[10,4])\n",
        "\n",
        "  # The first subplot will contain the predictions. Start by plotting the\n",
        "  # outputs (Y).\n",
        "  plt.subplot(121)\n",
        "  plt.xlabel('x')\n",
        "  plt.ylabel('y')\n",
        "  plt.xticks(inputs[:,1])\n",
        "  plt.scatter(inputs[:,1], outputs, color='black', label='Y')\n",
        "  \n",
        "  # For each epoch, retrieve the estimated weights W, compute predictions, and\n",
        "  # plot the resulting line.\n",
        "  num_epochs = len(weights)\n",
        "  for i in range(num_epochs):\n",
        "    W = weights[i]\n",
        "    predictions = np.dot(inputs, W.T)\n",
        "    plt.plot(inputs[:,1], predictions, label='Epoch %d' %i)\n",
        "  plt.legend()\n",
        "\n",
        "  # The second subplot will contain the losses.\n",
        "  plt.subplot(122)\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xticks(range(num_epochs))\n",
        "  plt.plot(range(num_epochs), losses, marker='o', color='black',\n",
        "           linestyle='dashed')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WECpzvgkrkFk"
      },
      "source": [
        "### Exercise 8: Plotting Progress (12.5 pts)\n",
        "\n",
        "Re-run gradient descent using X_with_1s, but this time with learning_rate=0.01 and num_epochs=7.\n",
        "\n",
        "Run the plot_learning function using the weights and losses returned by gradient_descent (from above) and answer the following questions:\n",
        "\n",
        "1. Is learning converging faster or slower than when we used learning_rate=0.02?\n",
        "2. If you continue training, will the loss eventually reach 0?\n",
        "3. If you continue training, will the model eventually converge to $h(x)=2x+1$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjfwzNhzrB1D"
      },
      "source": [
        "#### Student Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7CK6106tWgi"
      },
      "source": [
        "# YOUR CODE HERE\n",
        "\n",
        "weights, losses = gradient_descent(X_with_1s, Y, learning_rate=0.01, num_epochs=7)\n",
        "for W, loss in zip(weights, losses):\n",
        "  print(loss, W)\n",
        "plot_learning(X_with_1s, Y, weights, losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-mgY_e5upTL"
      },
      "source": [
        "WRITE YOUR ANSWERS HERE\n",
        "\n",
        "1. Although it may seem that this is a good model. With a learning rate of 0.02, which was on the previous exercises, we got a better result then this model when the learning rate is 0.01. The MSE is lower in the first model even though we only had 5 full passes of the data. In the previous model we had a MSE of 0.2045 and in this model the MSE is of 0.411.\n",
        "\n",
        "This would mean that the learning covergance is faster in the firs model (learning rate of 0.02) since we get to a line that has a MSE of 0.2045 in just 5 full passes.\n",
        "\n",
        "\n",
        "2. It will never reach 0. The data is not an actual line so there will always be an error more than 0. No matter if you continue training or if you have the best possible line. The points are not collinear.\n",
        "\n",
        "\n",
        "3. No. I tried to change the number of epochs from 7 to a bigger number and the first weight is actually not getting closser to 1, but something around 1.18\n",
        "The second weight does seem like is getting closer to 2. But no, that won't be the actual model is trying to approximate. "
      ]
    }
  ]
}