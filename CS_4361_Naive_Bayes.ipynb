{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of [Spring 2022] CS 4361 / 5361 - Naive Bayes",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Before you start**\n",
        "\n",
        "Make a copy of this Colab by clicking on File > Save a Copy in Drive\n",
        "\n",
        "After making a copy, add your student id, last name, and first name to the title."
      ],
      "metadata": {
        "id": "wUCyUM9O2xfA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "student_name = \"Salvador Robles Herrera\"\n",
        "student_id = \"80683116\""
      ],
      "metadata": {
        "id": "3Wjb-2xz2zxN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bs6gdwL7VSc1"
      },
      "source": [
        "# Exercises Naive Bayes Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-SBxroVVboY"
      },
      "source": [
        "## Naive Bayes - Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMZfZrcjSuWX"
      },
      "source": [
        "Consider a dataset with 3 classes and 5 binary attributes. Let the arrays p_class and p_att_given_class be:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9ElybXpS2vk",
        "outputId": "6c98f6a8-4a65-45d3-e89f-8a42d55c3361"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "p_class = np.ones(3)/3\n",
        "# Probabilities that a given attribute of a class is 1\n",
        "# Class 1 can be dogs, class 2 can be cat, class 3 can be like pandas\n",
        "p_att_given_class = np.array([[0.72, 0.21, 0.89, 0.47, 0.64],[0.32, 0.82, 0.54, 0.82, 0.17],[0.76, 0.65, 0.74, 0.31, 0.75]])\n",
        "\n",
        "print(p_class)\n",
        "print()\n",
        "print(p_att_given_class)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.33333333 0.33333333 0.33333333]\n",
            "\n",
            "[[0.72 0.21 0.89 0.47 0.64]\n",
            " [0.32 0.82 0.54 0.82 0.17]\n",
            " [0.76 0.65 0.74 0.31 0.75]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-zEYZGQTGc9"
      },
      "source": [
        "where p_class[i] represents the probability that an example belongs to class i and p_att_given_class[i,j] \n",
        "represents the probability that attribute j in an example of class i is equal to 1. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9Y4_l2HTOJc"
      },
      "source": [
        "What is the probability that for an example of class 2, attribute 0 is equal to 1?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zfx8C1JcTa9i",
        "outputId": "691f488d-6214-4643-8c47-1c341bae2688"
      },
      "source": [
        "print(p_att_given_class[2,0])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.76\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qwf-athTmc1"
      },
      "source": [
        "What is the probability that for an example of class 1, attribute 4 is equal to 0?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuCggzIqToud",
        "outputId": "a322c89b-9b86-465a-9ae8-9bae014f2c30"
      },
      "source": [
        "print(1-p_att_given_class[1,4])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.83\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_k8wCMxTbwV"
      },
      "source": [
        "How would the NaÃ¯ve Bayes classifier classify example [1,1,1,0,0]? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZDzTtoYTwBk"
      },
      "source": [
        "def classify(x,pc,pac):\n",
        "  p = pc \n",
        "  #print('x',x)\n",
        "  #print('p', p)\n",
        "  #print('pac', pac)\n",
        "  for i in range(x.shape[0]): # Number of attributes\n",
        "    if x[i]==1:\n",
        "      p = p* pac[:,i]\n",
        "    else:\n",
        "      p = p*(1-pac[:,i])\n",
        "\n",
        "  #print('pi: ', p)\n",
        "  #print('sum: ', np.sum(p))\n",
        "  if np.sum(p) != 0:\n",
        "    p = p/np.sum(p)\n",
        "  #print('probs ', p)\n",
        "  return np.argmax(p)\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfoDFaqKIAQL"
      },
      "source": [
        "def classify_no_loops(x,pc,pac):\n",
        "  p = x*pac + (1-x)*(1-pac)\n",
        "  p = np.prod(p, axis=1)\n",
        "  p = pc*p\n",
        "  p = p/np.sum(p)\n",
        "  print(p)\n",
        "  return np.argmax(p)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csF66Y2ZmU8l",
        "outputId": "452a3098-5d3e-4d1b-941d-ea34e882294a"
      },
      "source": [
        "test_example = np.array([1,1,1,0,0])\n",
        "classify(test_example,p_class,p_att_given_class)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_example = np.array([1,1,1,0,0])\n",
        "\n",
        "z = classify_no_loops(test_example, p_class, p_att_given_class)\n",
        "print('z', z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2wTTbY5-avDY",
        "outputId": "cde8ccf6-a6e1-49be-f62c-e6f1c8ffcdd4"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23361808 0.19261693 0.57376499]\n",
            "z 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGIf2Y1wVh81"
      },
      "source": [
        "## Exercise 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYvyltX-nUfE"
      },
      "source": [
        "Write a program to classify the MNIST dataset using the Naive Bayes classifer. \n",
        "\n",
        "Read the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVaf0APAVoLQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = np.float32(x_train/255).reshape(x_train.shape[0],-1)\n",
        "x_test = np.float32(x_test/255).reshape(x_test.shape[0],-1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a new dataset using binary attributes as follows:\n",
        "\n",
        "m = mean pixel value in x_train\n",
        "\n",
        "x_train_binary[i,j]  =  1 if x_train[i,j] >m; otherwise x_train_binary[i,j] = 0\n",
        "\n",
        "x_test_binary[i,j]  =  1 if x_test[i,j] >m; otherwise x_test_binary[i,j] = 0\n",
        "\n"
      ],
      "metadata": {
        "id": "iyFT71st15-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Note of assignment:**\n",
        "Note that in this assignment, I calculated the mean pixel of each attribute, instead of getting the overall mean value pixel of the whole training data.\n",
        "In the end I have 784 mean pixel values, which I will compare with the other values in each attributes of the examples provided in the training data."
      ],
      "metadata": {
        "id": "pV3rOrpDs6wC"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2g3AeGO1pgyy"
      },
      "source": [
        "# Note that the data is already in a 1-d array of size 784\n",
        "# The values from the y-value data goes from 0-9\n",
        "\n",
        "\n",
        "# Creating new dataset\n",
        "# Gets mean 784 mean values\n",
        "x_train_mean_pixel = x_train.sum(axis=0)/x_train.shape[0]\n",
        "x_test_mean_pixel = x_test.sum(axis=0)/x_test.shape[0]\n",
        "\n",
        "x_train_binary = np.zeros(shape=(x_train.shape[0], x_train.shape[1]))\n",
        "x_test_binary = np.zeros(shape=(x_test.shape[0], x_test.shape[1]))\n",
        "\n",
        "for i in range(x_train.shape[1]): # Number of attributes\n",
        "  x_train_binary[:,i] = np.where(x_train[:,i] <= x_train_mean_pixel[i], 0, 1)\n",
        "\n",
        "for j in range(x_test.shape[1]): # Number of attributes\n",
        "  x_test_binary[:,j] = np.where(x_test[:,j] <= x_test_mean_pixel[j], 0, 1)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8YhHybUbnvJr"
      },
      "source": [
        "Compute class probabilities (p_class) from training data, where p_class[i] represents the probability that an example belongs to class i. You can estimate this by dividing the number of instances in your training dataset that belong to class i over the number of training examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJsGkQ7hVo1F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a623edc-219b-469b-e08d-06990d10342a"
      },
      "source": [
        "# Your code here\n",
        "# Note that there are 10 classes (snekaers, dresses, etc)\n",
        "class_count_train = np.bincount(y_train)\n",
        "class_count_test = np.bincount(y_test)\n",
        "p_class_train = class_count_train / x_train_binary.shape[0]\n",
        "p_class_test = class_count_test / x_test_binary.shape[0]\n",
        "\n",
        "print(class_count_train)\n",
        "print(class_count_test)\n",
        "print(p_class_train)\n",
        "print(p_class_test)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5923 6742 5958 6131 5842 5421 5918 6265 5851 5949]\n",
            "[ 980 1135 1032 1010  982  892  958 1028  974 1009]\n",
            "[0.09871667 0.11236667 0.0993     0.10218333 0.09736667 0.09035\n",
            " 0.09863333 0.10441667 0.09751667 0.09915   ]\n",
            "[0.098  0.1135 0.1032 0.101  0.0982 0.0892 0.0958 0.1028 0.0974 0.1009]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svKdIXQWnxtr"
      },
      "source": [
        "Compute conditional probabilities (p_att_given_class) from training data, where p_att_given_class[i,j] represents the probability that attribute j in an example of class i is equal to 1. You can estimate this by dividing the number of times attribute j is equal to 1 in training instances of class j over the number of training instances of class j"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTRVw7xbVpfU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4710eb3d-63ac-4d6f-dadb-90c0cdf6eff3"
      },
      "source": [
        "# Your code here\n",
        "print(x_train_binary.shape)\n",
        "# Creates shape 10 classes rows and 784 columns of attributes\n",
        "p_att_given_class = np.zeros(shape=(p_class_train.shape[0], x_train_binary.shape[1]))\n",
        "\n",
        "for example in range(x_train_binary.shape[0]):\n",
        "  class_val = y_train[example]\n",
        "  for att in range(x_train_binary.shape[1]):\n",
        "    if x_train_binary[example][att] == 1:\n",
        "      p_att_given_class[class_val][att] += 1\n",
        "\n",
        "print('pagc', p_att_given_class)\n",
        "print('hello', p_att_given_class[:1])\n",
        "p_att_given_class = p_att_given_class / class_count_train[:, np.newaxis]\n",
        "print(p_att_given_class)\n",
        "print(np.argmax(p_att_given_class))\n",
        "\n",
        "sum = 0\n",
        "for i in range(p_att_given_class.shape[0]):\n",
        "  for j in range (p_att_given_class.shape[1]):\n",
        "    sum += p_att_given_class[i][j]\n",
        "\n",
        "print(sum)\n",
        "\n",
        "print('hwllo2', p_att_given_class[:1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 784)\n",
            "pagc [[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "hello [[0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00 1.000e+00\n",
            "  0.000e+00 1.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00 0.000e+00 1.000e+00\n",
            "  2.000e+00 1.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+00 4.000e+00\n",
            "  2.000e+00 2.000e+00 4.000e+00 2.000e+00 1.000e+00 1.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+00 2.000e+00 2.000e+00\n",
            "  3.000e+00 6.000e+00 1.700e+01 3.200e+01 5.500e+01 8.900e+01 1.200e+02\n",
            "  1.580e+02 1.750e+02 1.740e+02 1.460e+02 1.230e+02 9.400e+01 6.600e+01\n",
            "  4.100e+01 1.600e+01 9.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 2.000e+00 4.000e+00 6.000e+00 1.800e+01\n",
            "  3.600e+01 8.700e+01 1.830e+02 3.300e+02 6.100e+02 9.760e+02 1.372e+03\n",
            "  1.761e+03 2.050e+03 2.149e+03 1.994e+03 1.727e+03 1.338e+03 8.790e+02\n",
            "  5.280e+02 2.560e+02 9.500e+01 1.800e+01 2.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 3.000e+00 7.000e+00 1.800e+01 4.300e+01\n",
            "  1.060e+02 2.410e+02 4.910e+02 8.680e+02 1.408e+03 2.062e+03 2.732e+03\n",
            "  3.308e+03 3.710e+03 3.888e+03 3.734e+03 3.252e+03 2.641e+03 1.906e+03\n",
            "  1.213e+03 6.390e+02 2.510e+02 3.900e+01 2.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.500e+01 3.700e+01 8.500e+01\n",
            "  2.320e+02 4.790e+02 9.120e+02 1.552e+03 2.285e+03 2.993e+03 3.691e+03\n",
            "  4.161e+03 4.490e+03 4.616e+03 4.577e+03 4.298e+03 3.659e+03 2.869e+03\n",
            "  2.028e+03 1.171e+03 5.280e+02 1.020e+02 7.000e+00 2.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 1.500e+01 6.300e+01 1.680e+02\n",
            "  4.230e+02 8.880e+02 1.548e+03 2.306e+03 3.076e+03 3.731e+03 4.141e+03\n",
            "  4.341e+03 4.423e+03 4.465e+03 4.610e+03 4.603e+03 4.310e+03 3.695e+03\n",
            "  2.758e+03 1.806e+03 9.030e+02 2.180e+02 1.200e+01 1.000e+00 1.000e+00\n",
            "  1.000e+00 0.000e+00 2.000e+00 3.000e+00 1.700e+01 9.300e+01 3.290e+02\n",
            "  7.610e+02 1.405e+03 2.168e+03 3.025e+03 3.674e+03 4.074e+03 4.123e+03\n",
            "  3.998e+03 3.791e+03 3.684e+03 3.762e+03 4.114e+03 4.353e+03 4.153e+03\n",
            "  3.517e+03 2.451e+03 1.373e+03 3.830e+02 1.200e+01 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 2.000e+00 3.000e+00 3.000e+01 2.000e+02 5.820e+02\n",
            "  1.234e+03 2.028e+03 2.928e+03 3.653e+03 4.062e+03 4.069e+03 3.776e+03\n",
            "  3.232e+03 2.800e+03 2.524e+03 2.637e+03 3.182e+03 3.910e+03 4.305e+03\n",
            "  4.012e+03 3.079e+03 1.890e+03 5.820e+02 1.500e+01 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 3.000e+00 4.800e+01 3.830e+02 9.760e+02\n",
            "  1.795e+03 2.713e+03 3.558e+03 4.042e+03 4.118e+03 3.701e+03 3.008e+03\n",
            "  2.323e+03 1.718e+03 1.422e+03 1.573e+03 2.243e+03 3.306e+03 4.195e+03\n",
            "  4.291e+03 3.576e+03 2.376e+03 8.150e+02 2.400e+01 0.000e+00 0.000e+00\n",
            "  0.000e+00 1.000e+00 0.000e+00 1.000e+00 9.400e+01 6.440e+02 1.436e+03\n",
            "  2.443e+03 3.366e+03 4.026e+03 4.151e+03 3.737e+03 2.945e+03 2.112e+03\n",
            "  1.376e+03 8.560e+02 6.800e+02 9.120e+02 1.589e+03 2.800e+03 4.001e+03\n",
            "  4.487e+03 3.876e+03 2.719e+03 1.036e+03 3.400e+01 1.000e+00 0.000e+00\n",
            "  0.000e+00 1.000e+00 1.000e+00 1.000e+00 1.720e+02 1.020e+03 1.996e+03\n",
            "  3.101e+03 3.891e+03 4.215e+03 3.902e+03 3.063e+03 2.059e+03 1.195e+03\n",
            "  6.070e+02 3.600e+02 3.260e+02 5.700e+02 1.267e+03 2.546e+03 3.885e+03\n",
            "  4.496e+03 4.019e+03 2.844e+03 1.184e+03 5.000e+01 1.000e+00 0.000e+00\n",
            "  0.000e+00 1.000e+00 2.000e+00 3.000e+00 2.730e+02 1.418e+03 2.566e+03\n",
            "  3.662e+03 4.244e+03 4.153e+03 3.407e+03 2.272e+03 1.194e+03 5.040e+02\n",
            "  2.220e+02 1.460e+02 1.710e+02 4.410e+02 1.209e+03 2.460e+03 3.835e+03\n",
            "  4.455e+03 3.975e+03 2.839e+03 1.219e+03 5.900e+01 1.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 2.000e+00 4.280e+02 1.852e+03 3.117e+03\n",
            "  4.111e+03 4.367e+03 3.902e+03 2.753e+03 1.496e+03 5.940e+02 2.110e+02\n",
            "  1.060e+02 8.500e+01 1.380e+02 4.870e+02 1.363e+03 2.644e+03 3.894e+03\n",
            "  4.377e+03 3.826e+03 2.667e+03 1.164e+03 5.800e+01 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 5.000e+00 6.180e+02 2.257e+03 3.524e+03\n",
            "  4.398e+03 4.390e+03 3.484e+03 2.180e+03 9.690e+02 3.240e+02 9.800e+01\n",
            "  6.100e+01 9.100e+01 2.470e+02 7.800e+02 1.767e+03 3.014e+03 4.044e+03\n",
            "  4.204e+03 3.580e+03 2.421e+03 1.049e+03 6.000e+01 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 9.000e+00 8.040e+02 2.595e+03 3.781e+03\n",
            "  4.566e+03 4.275e+03 3.199e+03 1.767e+03 6.760e+02 2.150e+02 8.900e+01\n",
            "  9.700e+01 1.970e+02 5.430e+02 1.334e+03 2.412e+03 3.495e+03 4.163e+03\n",
            "  3.984e+03 3.194e+03 2.046e+03 8.550e+02 4.900e+01 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.400e+01 9.280e+02 2.734e+03 3.924e+03\n",
            "  4.617e+03 4.254e+03 3.026e+03 1.594e+03 6.450e+02 2.640e+02 1.980e+02\n",
            "  2.830e+02 5.920e+02 1.207e+03 2.152e+03 3.164e+03 3.917e+03 4.126e+03\n",
            "  3.580e+03 2.620e+03 1.601e+03 6.190e+02 3.100e+01 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.200e+01 9.730e+02 2.722e+03 3.902e+03\n",
            "  4.614e+03 4.338e+03 3.171e+03 1.863e+03 9.700e+02 6.210e+02 6.180e+02\n",
            "  8.740e+02 1.433e+03 2.218e+03 3.090e+03 3.813e+03 4.098e+03 3.778e+03\n",
            "  2.948e+03 2.017e+03 1.102e+03 3.910e+02 1.700e+01 0.000e+00 0.000e+00\n",
            "  0.000e+00 1.000e+00 1.000e+00 2.000e+01 9.360e+02 2.523e+03 3.708e+03\n",
            "  4.515e+03 4.463e+03 3.721e+03 2.634e+03 1.823e+03 1.496e+03 1.592e+03\n",
            "  1.989e+03 2.546e+03 3.262e+03 3.873e+03 4.085e+03 3.833e+03 3.119e+03\n",
            "  2.240e+03 1.412e+03 6.340e+02 1.990e+02 8.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 1.000e+00 1.000e+00 2.000e+01 7.660e+02 2.125e+03 3.308e+03\n",
            "  4.268e+03 4.648e+03 4.463e+03 3.790e+03 3.245e+03 2.954e+03 2.979e+03\n",
            "  3.254e+03 3.695e+03 4.046e+03 4.137e+03 3.793e+03 3.089e+03 2.288e+03\n",
            "  1.536e+03 7.960e+02 3.130e+02 7.100e+01 5.000e+00 1.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 1.500e+01 5.550e+02 1.549e+03 2.656e+03\n",
            "  3.695e+03 4.472e+03 4.869e+03 4.861e+03 4.601e+03 4.351e+03 4.241e+03\n",
            "  4.297e+03 4.342e+03 4.146e+03 3.639e+03 2.943e+03 2.196e+03 1.462e+03\n",
            "  8.080e+02 3.690e+02 1.180e+02 2.600e+01 2.000e+00 3.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 9.000e+00 2.950e+02 9.550e+02 1.807e+03\n",
            "  2.755e+03 3.671e+03 4.376e+03 4.784e+03 4.908e+03 4.812e+03 4.667e+03\n",
            "  4.349e+03 3.885e+03 3.256e+03 2.560e+03 1.860e+03 1.237e+03 7.020e+02\n",
            "  3.180e+02 1.300e+02 3.000e+01 1.400e+01 3.000e+00 3.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 8.300e+01 3.950e+02 9.240e+02\n",
            "  1.585e+03 2.299e+03 2.971e+03 3.511e+03 3.810e+03 3.811e+03 3.506e+03\n",
            "  3.046e+03 2.483e+03 1.895e+03 1.331e+03 8.460e+02 4.590e+02 2.100e+02\n",
            "  8.200e+01 2.500e+01 1.100e+01 5.000e+00 2.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.200e+01 6.300e+01 1.530e+02\n",
            "  3.310e+02 5.400e+02 7.820e+02 9.710e+02 1.112e+03 1.152e+03 1.063e+03\n",
            "  9.240e+02 7.080e+02 4.940e+02 3.190e+02 1.750e+02 8.200e+01 3.200e+01\n",
            "  1.000e+01 4.000e+00 2.000e+00 2.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  3.000e+00 1.000e+01 1.500e+01 1.700e+01 1.900e+01 1.800e+01 1.600e+01\n",
            "  1.200e+01 9.000e+00 5.000e+00 2.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00]]\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "1190\n",
            "1194.1073726454904\n",
            "hwllo2 [[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.68833361e-04\n",
            "  1.68833361e-04 1.68833361e-04 0.00000000e+00 1.68833361e-04\n",
            "  1.68833361e-04 1.68833361e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.68833361e-04\n",
            "  1.68833361e-04 0.00000000e+00 1.68833361e-04 3.37666723e-04\n",
            "  1.68833361e-04 0.00000000e+00 0.00000000e+00 1.68833361e-04\n",
            "  3.37666723e-04 6.75333446e-04 3.37666723e-04 3.37666723e-04\n",
            "  6.75333446e-04 3.37666723e-04 1.68833361e-04 1.68833361e-04\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.68833361e-04\n",
            "  3.37666723e-04 3.37666723e-04 3.37666723e-04 5.06500084e-04\n",
            "  1.01300017e-03 2.87016715e-03 5.40266757e-03 9.28583488e-03\n",
            "  1.50261692e-02 2.02600034e-02 2.66756711e-02 2.95458383e-02\n",
            "  2.93770049e-02 2.46496708e-02 2.07665035e-02 1.58703360e-02\n",
            "  1.11430019e-02 6.92216782e-03 2.70133378e-03 1.51950025e-03\n",
            "  1.68833361e-04 1.68833361e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.37666723e-04\n",
            "  6.75333446e-04 1.01300017e-03 3.03900051e-03 6.07800101e-03\n",
            "  1.46885024e-02 3.08965051e-02 5.57150093e-02 1.02988350e-01\n",
            "  1.64781361e-01 2.31639372e-01 2.97315550e-01 3.46108391e-01\n",
            "  3.62822894e-01 3.36653723e-01 2.91575215e-01 2.25899038e-01\n",
            "  1.48404525e-01 8.91440149e-02 4.32213405e-02 1.60391693e-02\n",
            "  3.03900051e-03 3.37666723e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 5.06500084e-04\n",
            "  1.18183353e-03 3.03900051e-03 7.25983454e-03 1.78963363e-02\n",
            "  4.06888401e-02 8.28971805e-02 1.46547358e-01 2.37717373e-01\n",
            "  3.48134391e-01 4.61252744e-01 5.58500760e-01 6.26371771e-01\n",
            "  6.56424109e-01 6.30423772e-01 5.49046092e-01 4.45888908e-01\n",
            "  3.21796387e-01 2.04794867e-01 1.07884518e-01 4.23771737e-02\n",
            "  6.58450110e-03 3.37666723e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  2.53250042e-03 6.24683437e-03 1.43508357e-02 3.91693399e-02\n",
            "  8.08711801e-02 1.53976026e-01 2.62029377e-01 3.85784231e-01\n",
            "  5.05318251e-01 6.23163937e-01 7.02515617e-01 7.58061793e-01\n",
            "  7.79334797e-01 7.72750295e-01 7.25645788e-01 6.17761270e-01\n",
            "  4.84382914e-01 3.42394057e-01 1.97703866e-01 8.91440149e-02\n",
            "  1.72210029e-02 1.18183353e-03 3.37666723e-04 1.68833361e-04\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.68833361e-04\n",
            "  2.53250042e-03 1.06365018e-02 2.83640047e-02 7.14165119e-02\n",
            "  1.49924025e-01 2.61354044e-01 3.89329732e-01 5.19331420e-01\n",
            "  6.29917272e-01 6.99138950e-01 7.32905622e-01 7.46749958e-01\n",
            "  7.53840959e-01 7.78321796e-01 7.77139963e-01 7.27671788e-01\n",
            "  6.23839271e-01 4.65642411e-01 3.04913051e-01 1.52456525e-01\n",
            "  3.68056728e-02 2.02600034e-03 1.68833361e-04 1.68833361e-04\n",
            "  1.68833361e-04 0.00000000e+00 3.37666723e-04 5.06500084e-04\n",
            "  2.87016715e-03 1.57015026e-02 5.55461759e-02 1.28482188e-01\n",
            "  2.37210873e-01 3.66030728e-01 5.10720918e-01 6.20293770e-01\n",
            "  6.87827115e-01 6.96099949e-01 6.74995779e-01 6.40047273e-01\n",
            "  6.21982104e-01 6.35151106e-01 6.94580449e-01 7.34931622e-01\n",
            "  7.01164950e-01 5.93786932e-01 4.13810569e-01 2.31808205e-01\n",
            "  6.46631774e-02 2.02600034e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 3.37666723e-04 5.06500084e-04\n",
            "  5.06500084e-03 3.37666723e-02 9.82610164e-02 2.08340368e-01\n",
            "  3.42394057e-01 4.94344082e-01 6.16748269e-01 6.85801114e-01\n",
            "  6.86982948e-01 6.37514773e-01 5.45669424e-01 4.72733412e-01\n",
            "  4.26135404e-01 4.45213574e-01 5.37227756e-01 6.60138443e-01\n",
            "  7.26827621e-01 6.77359446e-01 5.19837920e-01 3.19095053e-01\n",
            "  9.82610164e-02 2.53250042e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.68833361e-04 5.06500084e-04\n",
            "  8.10400135e-03 6.46631774e-02 1.64781361e-01 3.03055884e-01\n",
            "  4.58044910e-01 6.00709100e-01 6.82424447e-01 6.95255783e-01\n",
            "  6.24852271e-01 5.07850751e-01 3.92199899e-01 2.90055715e-01\n",
            "  2.40081040e-01 2.65574878e-01 3.78693230e-01 5.58163093e-01\n",
            "  7.08255951e-01 7.24463954e-01 6.03748101e-01 4.01148067e-01\n",
            "  1.37599190e-01 4.05200068e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.68833361e-04 0.00000000e+00 1.68833361e-04\n",
            "  1.58703360e-02 1.08728685e-01 2.42444707e-01 4.12459902e-01\n",
            "  5.68293095e-01 6.79723113e-01 7.00827283e-01 6.30930272e-01\n",
            "  4.97214250e-01 3.56576059e-01 2.32314705e-01 1.44521357e-01\n",
            "  1.14806686e-01 1.53976026e-01 2.68276211e-01 4.72733412e-01\n",
            "  6.75502279e-01 7.57555293e-01 6.54398109e-01 4.59057910e-01\n",
            "  1.74911362e-01 5.74033429e-03 1.68833361e-04 0.00000000e+00\n",
            "  0.00000000e+00 1.68833361e-04 1.68833361e-04 1.68833361e-04\n",
            "  2.90393382e-02 1.72210029e-01 3.36991389e-01 5.23552254e-01\n",
            "  6.56930609e-01 7.11632619e-01 6.58787776e-01 5.17136586e-01\n",
            "  3.47627891e-01 2.01755867e-01 1.02481850e-01 6.07800101e-02\n",
            "  5.50396758e-02 9.62350160e-02 2.13911869e-01 4.29849738e-01\n",
            "  6.55917609e-01 7.59074793e-01 6.78541280e-01 4.80162080e-01\n",
            "  1.99898700e-01 8.44166807e-03 1.68833361e-04 0.00000000e+00\n",
            "  0.00000000e+00 1.68833361e-04 3.37666723e-04 5.06500084e-04\n",
            "  4.60915077e-02 2.39405707e-01 4.33226406e-01 6.18267770e-01\n",
            "  7.16528786e-01 7.01164950e-01 5.75215263e-01 3.83589397e-01\n",
            "  2.01587034e-01 8.50920142e-02 3.74810062e-02 2.46496708e-02\n",
            "  2.88705048e-02 7.44555124e-02 2.04119534e-01 4.15330069e-01\n",
            "  6.47475941e-01 7.52152625e-01 6.71112612e-01 4.79317913e-01\n",
            "  2.05807868e-01 9.96116833e-03 1.68833361e-04 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 3.37666723e-04\n",
            "  7.22606787e-02 3.12679385e-01 5.26253588e-01 6.94073949e-01\n",
            "  7.37295290e-01 6.58787776e-01 4.64798244e-01 2.52574709e-01\n",
            "  1.00287017e-01 3.56238393e-02 1.78963363e-02 1.43508357e-02\n",
            "  2.32990039e-02 8.22218470e-02 2.30119872e-01 4.46395408e-01\n",
            "  6.57437110e-01 7.38983623e-01 6.45956441e-01 4.50278575e-01\n",
            "  1.96522033e-01 9.79233497e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 8.44166807e-04\n",
            "  1.04339017e-01 3.81056897e-01 5.94968766e-01 7.42529124e-01\n",
            "  7.41178457e-01 5.88215431e-01 3.68056728e-01 1.63599527e-01\n",
            "  5.47020091e-02 1.65456694e-02 1.02988350e-02 1.53638359e-02\n",
            "  4.17018403e-02 1.31690022e-01 2.98328550e-01 5.08863751e-01\n",
            "  6.82762114e-01 7.09775452e-01 6.04423434e-01 4.08745568e-01\n",
            "  1.77106196e-01 1.01300017e-02 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.51950025e-03\n",
            "  1.35742023e-01 4.38122573e-01 6.38358940e-01 7.70893128e-01\n",
            "  7.21762620e-01 5.40097923e-01 2.98328550e-01 1.14131352e-01\n",
            "  3.62991727e-02 1.50261692e-02 1.63768361e-02 3.32601722e-02\n",
            "  9.16765153e-02 2.25223704e-01 4.07226068e-01 5.90072598e-01\n",
            "  7.02853284e-01 6.72632112e-01 5.39253757e-01 3.45433058e-01\n",
            "  1.44352524e-01 8.27283471e-03 0.00000000e+00 1.68833361e-04\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.36366706e-03\n",
            "  1.56677359e-01 4.61590410e-01 6.62502110e-01 7.79503630e-01\n",
            "  7.18217120e-01 5.10889752e-01 2.69120378e-01 1.08897518e-01\n",
            "  4.45720074e-02 3.34290056e-02 4.77798413e-02 9.99493500e-02\n",
            "  2.03781867e-01 3.63329394e-01 5.34188756e-01 6.61320277e-01\n",
            "  6.96606449e-01 6.04423434e-01 4.42343407e-01 2.70302212e-01\n",
            "  1.04507851e-01 5.23383421e-03 0.00000000e+00 1.68833361e-04\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 2.02600034e-03\n",
            "  1.64274861e-01 4.59564410e-01 6.58787776e-01 7.78997130e-01\n",
            "  7.32399122e-01 5.35370589e-01 3.14536552e-01 1.63768361e-01\n",
            "  1.04845517e-01 1.04339017e-01 1.47560358e-01 2.41938207e-01\n",
            "  3.74472396e-01 5.21695087e-01 6.43761607e-01 6.91879115e-01\n",
            "  6.37852440e-01 4.97720750e-01 3.40536890e-01 1.86054364e-01\n",
            "  6.60138443e-02 2.87016715e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.68833361e-04 1.68833361e-04 3.37666723e-03\n",
            "  1.58028026e-01 4.25966571e-01 6.26034104e-01 7.62282627e-01\n",
            "  7.53503292e-01 6.28228938e-01 4.44707074e-01 3.07783218e-01\n",
            "  2.52574709e-01 2.68782711e-01 3.35809556e-01 4.29849738e-01\n",
            "  5.50734425e-01 6.53891609e-01 6.89684282e-01 6.47138275e-01\n",
            "  5.26591254e-01 3.78186730e-01 2.38392706e-01 1.07040351e-01\n",
            "  3.35978389e-02 1.35066689e-03 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 1.68833361e-04 1.68833361e-04 3.37666723e-03\n",
            "  1.29326355e-01 3.58770893e-01 5.58500760e-01 7.20580787e-01\n",
            "  7.84737464e-01 7.53503292e-01 6.39878440e-01 5.47864258e-01\n",
            "  4.98733750e-01 5.02954584e-01 5.49383758e-01 6.23839271e-01\n",
            "  6.83099781e-01 6.98463616e-01 6.40384940e-01 5.21526254e-01\n",
            "  3.86290731e-01 2.59328043e-01 1.34391356e-01 5.28448421e-02\n",
            "  1.19871687e-02 8.44166807e-04 1.68833361e-04 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.68833361e-04 2.53250042e-03\n",
            "  9.37025156e-02 2.61522877e-01 4.48421408e-01 6.23839271e-01\n",
            "  7.55022793e-01 8.22049637e-01 8.20698970e-01 7.76802296e-01\n",
            "  7.34593956e-01 7.16022286e-01 7.25476954e-01 7.33074456e-01\n",
            "  6.99983117e-01 6.14384602e-01 4.96876583e-01 3.70758062e-01\n",
            "  2.46834374e-01 1.36417356e-01 6.22995104e-02 1.99223367e-02\n",
            "  4.38966740e-03 3.37666723e-04 5.06500084e-04 1.68833361e-04\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.51950025e-03\n",
            "  4.98058416e-02 1.61235860e-01 3.05081884e-01 4.65135911e-01\n",
            "  6.19787270e-01 7.38814790e-01 8.07698801e-01 8.28634138e-01\n",
            "  8.12426135e-01 7.87945298e-01 7.34256289e-01 6.55917609e-01\n",
            "  5.49721425e-01 4.32213405e-01 3.14030052e-01 2.08846868e-01\n",
            "  1.18521020e-01 5.36890089e-02 2.19483370e-02 5.06500084e-03\n",
            "  2.36366706e-03 5.06500084e-04 5.06500084e-04 1.68833361e-04\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 1.68833361e-04\n",
            "  1.40131690e-02 6.66891778e-02 1.56002026e-01 2.67600878e-01\n",
            "  3.88147898e-01 5.01603917e-01 5.92773932e-01 6.43255107e-01\n",
            "  6.43423941e-01 5.91929765e-01 5.14266419e-01 4.19213237e-01\n",
            "  3.19939220e-01 2.24717204e-01 1.42833024e-01 7.74945129e-02\n",
            "  3.54550059e-02 1.38443356e-02 4.22083404e-03 1.85716698e-03\n",
            "  8.44166807e-04 3.37666723e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  2.02600034e-03 1.06365018e-02 2.58315043e-02 5.58838426e-02\n",
            "  9.11700152e-02 1.32027689e-01 1.63937194e-01 1.87742698e-01\n",
            "  1.94496032e-01 1.79469863e-01 1.56002026e-01 1.19534020e-01\n",
            "  8.34036806e-02 5.38578423e-02 2.95458383e-02 1.38443356e-02\n",
            "  5.40266757e-03 1.68833361e-03 6.75333446e-04 3.37666723e-04\n",
            "  3.37666723e-04 1.68833361e-04 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 1.68833361e-04 5.06500084e-04\n",
            "  1.68833361e-03 2.53250042e-03 2.87016715e-03 3.20783387e-03\n",
            "  3.03900051e-03 2.70133378e-03 2.02600034e-03 1.51950025e-03\n",
            "  8.44166807e-04 3.37666723e-04 1.68833361e-04 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohcp-tEJn5bz"
      },
      "source": [
        "Classify test data - print accuracy and confusion matrix (feel free to use sklearn to print the accuracy and confusion matrix)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7E2QCH-7Vrw1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "037b92bb-ec36-456c-a521-218d0fe00bfe"
      },
      "source": [
        "# Your code here\n",
        "# Importing confusion matrix formula\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def accuracy_multiclass(preds, Y_test):\n",
        "  return np.sum(preds == Y_test) / Y_test.shape[0]\n",
        "\n",
        "preds = np.zeros(y_test.shape[0])\n",
        "print('binary shape', x_test_binary.shape)\n",
        "# Going through testing data x_test\n",
        "for i in range(y_test.shape[0]):\n",
        "  class_pred = classify(x_test_binary[i], p_class_train, p_att_given_class)\n",
        "  preds[i] = class_pred\n",
        "\n",
        "accuracy = accuracy_multiclass(preds, y_test)\n",
        "\n",
        "print('preds', preds)\n",
        "print('y_test', y_test)\n",
        "print('Accuracy: ', accuracy)\n",
        "print(confusion_matrix(preds, y_test))\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "binary shape (10000, 784)\n",
            "preds [7. 2. 1. ... 9. 8. 6.]\n",
            "y_test [7 2 1 ... 4 5 6]\n",
            "Accuracy:  0.8373\n",
            "[[ 875    0   19    5    2   18   18    1   13   10]\n",
            " [   0 1082   11   19    8    9   18   29   31   15]\n",
            " [   3    7  846   36    8    8   19   13   11    5]\n",
            " [   4    4   33  846    1  121    1    6   71    9]\n",
            " [   1    0   17    1  769   23    9   16   16   71]\n",
            " [  43    4    4   15    2  635   31    2   23    9]\n",
            " [  28    5   30    8   16   19  853    0    7    0]\n",
            " [   1    0   16   16    1    8    0  862    5   27]\n",
            " [  25   33   54   41   20   27    9   28  757   15]\n",
            " [   0    0    2   23  155   24    0   71   40  848]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Note on assignment:**\n",
        "As previously noted, in this assignment I calculated the mean pixel value for each attribute, so in total 784 mean pixel values. This way I achieved better accuracy than if I would have had only one mean pixel value and compare that to every other pixel values.\n",
        "Also, just for a fun fact I tried 0.5 instead of the mean pixel value, and that actually got a better accuracy 0.84 than trying the previous mean pixel values."
      ],
      "metadata": {
        "id": "A34AYsDkpOES"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submission Instructions\n",
        "\n",
        "1. File > Download .ipynb\n",
        "2. Go to Blackboard, find the submission page, and upload the .ipynb file you just downloaded."
      ],
      "metadata": {
        "id": "THpbziuF23A4"
      }
    }
  ]
}